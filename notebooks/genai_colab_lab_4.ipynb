{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSVyZRkvmqyc"
      },
      "source": [
        "# Setup Environment\n",
        "The following code loads the environment variables required to run this notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwWfBNdUmqyd"
      },
      "outputs": [],
      "source": [
        "FILE=\"GenAI Lab 4\"\n",
        "\n",
        "! pip install -qqq git+https://github.com/elastic/notebook-workshop-loader.git@main\n",
        "from notebookworkshoploader import loader\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "if os.path.isfile(\"../env\"):\n",
        "    load_dotenv(\"../env\", override=True)\n",
        "    print('Successfully loaded environment variables from local env file')\n",
        "else:\n",
        "    loader.load_remote_env(file=FILE, env_url=\"https://notebook-workshop-api-voldmqr2bq-uc.a.run.app\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovmMH14E_cZv"
      },
      "outputs": [],
      "source": [
        "### TODO Custom connection strings sill required not ready for prod\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ln-8SRvAI-jS"
      },
      "outputs": [],
      "source": [
        "! pip install -qqq  openai==0.28.1 # tiktoken==0.5.2 cohere==4.38\n",
        "! pip install -qqq streamlit==1.30.0 elasticsearch==8.12.0 elastic-apm==6.20.0 inquirer==3.2.1 python-dotenv==1.0.1\n",
        "\n",
        "import os, inquirer, re, secrets, requests\n",
        "import streamlit as st\n",
        "import openai\n",
        "\n",
        "from IPython.display import display\n",
        "from ipywidgets import widgets\n",
        "from pprint import pprint\n",
        "from elasticsearch import Elasticsearch\n",
        "from string import Template\n",
        "from requests.auth import HTTPBasicAuth\n",
        "\n",
        "#if using the Elastic AI proxy, then generate the correct API key\n",
        "if os.environ['ELASTIC_PROXY'] == \"True\":\n",
        "\n",
        "  #remove the api type variable: it's a must when using the proxy\n",
        "  if \"OPENAI_API_TYPE\" in os.environ: del os.environ[\"OPENAI_API_TYPE\"]\n",
        "\n",
        "  #generate and share \"your\" unique hash\n",
        "  os.environ['USER_HASH'] = secrets.token_hex(nbytes=6)\n",
        "  print(f\"Your unique user hash is: {os.environ['USER_HASH']}\")\n",
        "\n",
        "  #get the current API key and combine with your hash\n",
        "  os.environ['OPENAI_API_KEY'] = f\"{os.environ['OPENAI_API_KEY']} {os.environ['USER_HASH']}\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEOV5KlycEmV"
      },
      "source": [
        "## Create Elasticsearch client connection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUIB9n9ccGAR"
      },
      "outputs": [],
      "source": [
        "if 'ELASTIC_CLOUD_ID' in os.environ:\n",
        "  es = Elasticsearch(\n",
        "    cloud_id=os.environ['ELASTIC_CLOUD_ID'],\n",
        "    api_key=(os.environ['ELASTIC_APIKEY_ID'], os.environ['ELASTIC_APIKEY_SECRET']),\n",
        "    request_timeout=30\n",
        "  )\n",
        "elif 'ELASTIC_URL' in os.environ:\n",
        "  es = Elasticsearch(\n",
        "    os.environ['ELASTIC_URL'],\n",
        "    api_key=(os.environ['ELASTIC_APIKEY_ID'], os.environ['ELASTIC_APIKEY_SECRET']),\n",
        "    request_timeout=30\n",
        "  )\n",
        "else:\n",
        "  print(\"env needs to set either ELASTIC_CLOUD_ID or ELASTIC_URL\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCsxLt8Yc2LI"
      },
      "source": [
        "# Lab 4-1\n",
        "RAG Lab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbZ7VUSp8P32"
      },
      "source": [
        "## Step 1: Write the Streamlit single page app to the file system"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WyaMBLbUWSU"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import os\n",
        "\n",
        "import streamlit as st\n",
        "import openai\n",
        "from elasticsearch import Elasticsearch\n",
        "import elasticapm\n",
        "import base64\n",
        "\n",
        "######################################\n",
        "# Streamlit Configuration\n",
        "st.set_page_config(layout=\"wide\")\n",
        "\n",
        "\n",
        "import textwrap\n",
        "# wrap text when printing, because colab scrolls output to the right too much\n",
        "def wrap_text(text, width):\n",
        "    wrapped_text = textwrap.wrap(text, width)\n",
        "    return '\\n'.join(wrapped_text)\n",
        "\n",
        "@st.cache_data()\n",
        "def get_base64(bin_file):\n",
        "    with open(bin_file, 'rb') as f:\n",
        "        data = f.read()\n",
        "    return base64.b64encode(data).decode()\n",
        "\n",
        "\n",
        "def set_background(png_file):\n",
        "    bin_str = get_base64(png_file)\n",
        "    page_bg_img = '''\n",
        "    <style>\n",
        "    .stApp {\n",
        "    background-image: url(\"data:image/png;base64,%s\");\n",
        "    background-size: cover;\n",
        "    }\n",
        "    </style>\n",
        "    ''' % bin_str\n",
        "    st.markdown(page_bg_img, unsafe_allow_html=True)\n",
        "    return\n",
        "\n",
        "\n",
        "#set_background('images/smoke_blackyellow1.png')\n",
        "\n",
        "######################################\n",
        "\n",
        "######################################\n",
        "# Sidebar Options\n",
        "def sidebar_bg(side_bg):\n",
        "    side_bg_ext = 'png'\n",
        "    st.markdown(\n",
        "        f\"\"\"\n",
        "      <style>\n",
        "      [data-testid=\"stSidebar\"] > div:first-child {{\n",
        "          background: url(data:image/{side_bg_ext};base64,{base64.b64encode(open(side_bg, \"rb\").read()).decode()});\n",
        "      }}\n",
        "      </style>\n",
        "      \"\"\",\n",
        "        unsafe_allow_html=True,\n",
        "    )\n",
        "\n",
        "\n",
        "side_bg = './images/sidebar_chairs3.jpg'\n",
        "sidebar_bg(side_bg)\n",
        "\n",
        "# sidebar logo\n",
        "st.markdown(\n",
        "    \"\"\"\n",
        "    <style>\n",
        "        [data-testid=stSidebar] [data-testid=stImage]{\n",
        "            text-align: center;\n",
        "            display: block;\n",
        "            margin-left: auto;\n",
        "            margin-right: auto;\n",
        "            width: 100%;\n",
        "        }\n",
        "    </style>\n",
        "    \"\"\", unsafe_allow_html=True\n",
        ")\n",
        "\n",
        "with st.sidebar:\n",
        "    st.image(\"images/elastic_logo_transp_100.png\")\n",
        "\n",
        "######################################\n",
        "\n",
        "\n",
        "# Configure OpenAI client\n",
        "openai.api_key = os.environ['OPENAI_API_KEY']\n",
        "openai.api_base = os.environ['OPENAI_API_BASE']\n",
        "openai.default_model = os.environ['OPENAI_API_ENGINE']\n",
        "openai.verify_ssl_certs = False\n",
        "\n",
        "\n",
        "# Initialize Elasticsearch and APM clients\n",
        "# Configure APM and Elasticsearch clients\n",
        "@st.cache_resource\n",
        "def initElastic():\n",
        "    os.environ['ELASTIC_APM_SERVICE_NAME'] = \"genai_workshop_v2_lab_2-2\"\n",
        "    apmclient = elasticapm.Client()\n",
        "    elasticapm.instrument()\n",
        "\n",
        "    if 'ELASTIC_CLOUD_ID' in os.environ:\n",
        "        es = Elasticsearch(\n",
        "            cloud_id=os.environ['ELASTIC_CLOUD_ID'],\n",
        "            basic_auth=(os.environ['ELASTIC_USER'], os.environ['ELASTIC_PASSWORD']),\n",
        "            request_timeout=30\n",
        "        )\n",
        "    else:\n",
        "        es = Elasticsearch(\n",
        "            os.environ['ELASTIC_URL'],\n",
        "            basic_auth=(os.environ['ELASTIC_USER'], os.environ['ELASTIC_PASSWORD']),\n",
        "            request_timeout=30\n",
        "        )\n",
        "\n",
        "    if os.environ['ELASTIC_PROXY'] != \"True\":\n",
        "        openai.api_type = os.environ['OPENAI_API_TYPE']\n",
        "        openai.api_version = os.environ['OPENAI_API_VERSION']\n",
        "\n",
        "    return apmclient, es\n",
        "\n",
        "\n",
        "apmclient, es = initElastic()\n",
        "\n",
        "# Set our data index\n",
        "index = os.environ['ELASTIC_INDEX_DOCS']\n",
        "\n",
        "\n",
        "# Run an Elasticsearch query using BM25 relevance scoring\n",
        "@elasticapm.capture_span(\"bm25_search\")\n",
        "def search_bm25(query_text, es, size=1, augment_method=\"Full Text\"):\n",
        "    if augment_method == \"Full Text\":\n",
        "        query = {\n",
        "            \"bool\": {\n",
        "                \"must\": {\n",
        "                    \"match\": {\n",
        "                    \"text\": query_text\n",
        "                    }\n",
        "                },\n",
        "                \"filter\": {\n",
        "                    \"term\": {\n",
        "                    \"categories.keyword\": \"Living people\"\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "    elif augment_method == \"Matching Chunk\":\n",
        "        query = {\n",
        "            \"query\": {\n",
        "                \"bool\": {\n",
        "                    \"must\": [\n",
        "                        {\n",
        "                        \"nested\": {\n",
        "                            \"path\": \"passages\",\n",
        "                            \"query\": {\n",
        "                            \"bool\": {\n",
        "                                \"must\": [\n",
        "                                {\n",
        "                                    \"match\": {\n",
        "                                    \"passages.text\": query_text\n",
        "                                    }\n",
        "                                }\n",
        "                                ]\n",
        "                            }\n",
        "                            },\n",
        "                            \"inner_hits\": {\n",
        "                            \"_source\": False,\n",
        "                            \"fields\": [\n",
        "                                \"passages.text\"\n",
        "                            ]\n",
        "                            }\n",
        "                        }\n",
        "                        }\n",
        "                    ],\n",
        "                    \"filter\": {\n",
        "                        \"term\": {\n",
        "                        \"categories.keyword\": \"Living people\"\n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    fields = [\n",
        "        \"text\",\n",
        "        \"title\",\n",
        "    ]\n",
        "\n",
        "    resp = es.search(index=index,\n",
        "                     query=query,\n",
        "                     fields=fields,\n",
        "                     size=size,\n",
        "                     source=False)\n",
        "    # print(resp)\n",
        "    body = resp\n",
        "    url = 'nothing'\n",
        "\n",
        "    return body, url\n",
        "\n",
        "\n",
        "@elasticapm.capture_span(\"knn_search\")\n",
        "def search_knn(query_text, es, size=1, augment_method=\"Full Text\"):\n",
        "    fields = [\n",
        "        \"title\",\n",
        "        \"text\"\n",
        "    ]\n",
        "\n",
        "    knn = {\n",
        "        \"inner_hits\": {\n",
        "            \"_source\": False,\n",
        "            \"fields\": [\n",
        "                \"passages.text\"\n",
        "            ]\n",
        "        },\n",
        "        \"field\": \"passages.embeddings\",\n",
        "        \"k\": size,\n",
        "        \"num_candidates\": 100,\n",
        "        \"query_vector_builder\": {\n",
        "            \"text_embedding\": {\n",
        "                \"model_id\": \"sentence-transformers__all-distilroberta-v1\",\n",
        "                \"model_text\": query_text\n",
        "            }\n",
        "        },\n",
        "        \"filter\": {\n",
        "            \"term\": {\n",
        "            \"categories.keyword\": \"Living people\"\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    resp = es.search(index=index,\n",
        "                     knn=knn,\n",
        "                     fields=fields,\n",
        "                     size=size,\n",
        "                     source=False)\n",
        "\n",
        "    # body = resp['hits']['hits'][0]['fields']['body_content'][0]\n",
        "    # url = resp['hits']['hits'][0]['fields']['url'][0]\n",
        "\n",
        "    return resp, None\n",
        "\n",
        "\n",
        "def truncate_text(text, max_tokens):\n",
        "    tokens = text.split()\n",
        "    if len(tokens) <= max_tokens:\n",
        "        return text\n",
        "\n",
        "    return ' '.join(tokens[:max_tokens])\n",
        "\n",
        "\n",
        "def build_text_obj(resp, aug_method):\n",
        "    '''\n",
        "    parse the response from ES and return a dict of the text fields\n",
        "    :param resp:\n",
        "    :return:\n",
        "    '''\n",
        "\n",
        "    # print(resp)\n",
        "    tobj = {}\n",
        "\n",
        "    for hit in resp['hits']['hits']:\n",
        "        # tobj[hit['fields']['title'][0]] = []\n",
        "        title = hit['fields']['title'][0]\n",
        "        tobj.setdefault(title, [])\n",
        "\n",
        "        if aug_method == \"Matching Chunk\":\n",
        "            # print('hit')\n",
        "            # print(hit)\n",
        "            # tobj['passages'] = []\n",
        "            for ihit in hit['inner_hits']['passages']['hits']['hits']:\n",
        "                tobj[title].append(\n",
        "                    {'passage': ihit['fields']['passages'][0]['text'][0],\n",
        "                     '_score': ihit['_score']}\n",
        "                )\n",
        "        elif aug_method == \"Full Text\":\n",
        "            tobj[title].append(\n",
        "                hit['fields']\n",
        "            )\n",
        "\n",
        "    return tobj\n",
        "\n",
        "def build_text_summary(resp):\n",
        "    response = [\"Titles of Documents Retrieved: \"]\n",
        "    for hit in resp['hits']['hits']:\n",
        "        title = hit['fields']['title'][0]\n",
        "        response.append( f\" * {title}\" )\n",
        "    return \"\\n\".join(response)\n",
        "\n",
        "\n",
        "def generate_response(query, es, search_method, custom_prompt, negative_response, show_prompt, size=1,\n",
        "                      augment_method=\"Full Text\"):\n",
        "    \"\"\"\n",
        "    Generates a response from ChatGPT based on the given query and Search Method.\n",
        "    Formats the prompt, sends it to ChatGPT, and displays the results.\n",
        "    \"\"\"\n",
        "\n",
        "    # Perform the search based on the specified method\n",
        "    search_functions = {\n",
        "        'bm25': {'method': search_bm25, 'display': 'Lexical Search'},\n",
        "        'knn': {'method': search_knn, 'display': 'Semantic Search'}\n",
        "    }\n",
        "    search_func = search_functions.get(search_method)['method']\n",
        "    if not search_func:\n",
        "        raise ValueError(f\"Invalid search method: {search_method}\")\n",
        "\n",
        "    # Perform the search and format the docs\n",
        "    response, url = search_func(query, es, size, augment_method)\n",
        "    augment_text = build_text_obj(response, augment_method)\n",
        "    augment_summary = build_text_summary(response)\n",
        "\n",
        "    res_col1, res_col2 = st.columns(2)\n",
        "    # Display the search results from ES\n",
        "    with res_col2:\n",
        "        st.subheader(':rainbow[Elasticsearch Response]')\n",
        "        st.write(':gray[Search Method:] :blue[%s]' % search_functions.get(search_method)['display'])\n",
        "        st.write(':gray[Size Setting:] :blue[%s]' % size)\n",
        "        st.write(':gray[Augment Setting:] :blue[%s]' % augment_method)\n",
        "\n",
        "        st.write(':green[Augment Chunk(s) from Elasticsearch]')\n",
        "        st.write(str(augment_summary))\n",
        "        st.json(dict(augment_text))\n",
        "\n",
        "        st.write(':violet[Elasticsearch Response]')\n",
        "\n",
        "        st.json(dict(response))\n",
        "\n",
        "    #    response_text = response['hits']['hits'][0]['fields']['text'][0]\n",
        "    formatted_prompt = custom_prompt.replace(\"$query\", query).replace(\"$response\", str(augment_text)).replace(\n",
        "        \"$negResponse\", negative_response)\n",
        "\n",
        "    # Generate the ChatGPT response\n",
        "\n",
        "    with res_col1:\n",
        "        st.subheader(':orange[GenAI Response]')\n",
        "\n",
        "        chat_response = chat_gpt(formatted_prompt, system_prompt=\"You are a helpful assistant.\")\n",
        "        st.markdown(chat_response)\n",
        "\n",
        "    # Display results\n",
        "    if show_prompt:\n",
        "        st.text(\"Full prompt sent to ChatGPT:\")\n",
        "        st.text(wrap_text(formatted_prompt,70))\n",
        "\n",
        "    #    st.header('Response from ChatGPT')\n",
        "    #    st.text(chat_response)\n",
        "\n",
        "    if negative_response not in chat_response:\n",
        "        st.text(\"Reference URL:\")\n",
        "        st.text(url)\n",
        "\n",
        "\n",
        "def chat_gpt(user_prompt, system_prompt):\n",
        "    \"\"\"\n",
        "    Generates a response from ChatGPT based on the given user and system prompts.\n",
        "    \"\"\"\n",
        "    max_tokens = 1024\n",
        "    max_context_tokens = 4000\n",
        "    safety_margin = 5\n",
        "\n",
        "    # Truncate the prompt content to fit within the model's context length\n",
        "    truncated_prompt = truncate_text(user_prompt, max_context_tokens - max_tokens - safety_margin)\n",
        "\n",
        "    # Prepare the messages for the ChatGPT API\n",
        "    messages = [{\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": truncated_prompt}]\n",
        "\n",
        "    # Make the OpenAI API call\n",
        "    response = openai.ChatCompletion.create(model=openai.default_model, temperature=0, messages=messages)\n",
        "\n",
        "    # Add APM metadata and return the response content\n",
        "    elasticapm.set_custom_context({'model': openai.default_model, 'prompt': user_prompt})\n",
        "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "\n",
        "# Main chat form\n",
        "st.title(\"Search Powered AI - Famous Living People:\")\n",
        "\n",
        "# Define the default prompt and negative response\n",
        "default_prompt_intro = \"You are a helpful AI assistant that provides concise answers to questions about famous people using the provided context. \"\n",
        "default_response_instructions = \"Context: $response\\n\"\n",
        "default_negative_response = \"If you cannot answer the question using only the provided context, reply with 'I'm unable to answer the question based on the information I have from wikipedia' and nothing else.\"\n",
        "\n",
        "\n",
        "with st.form(\"chat_form\"):\n",
        "\n",
        "    query = st.text_input(f\"Ask a question about a famous living person :\",\n",
        "                          placeholder='Who is Bill Gates and what was his biggest philanthropic act?')\n",
        "\n",
        "    opt_col1, opt_col2 = st.columns(2)\n",
        "    with opt_col1:\n",
        "        with st.expander(\"Customize Prompt Template\"):\n",
        "            prompt_intro = st.text_area(\"Introduction/context of the prompt:\", value=default_prompt_intro)\n",
        "            prompt_query_placeholder = st.text_area(\"Placeholder for the user's query:\", value=\"Answer this question: $query\")\n",
        "            prompt_response_placeholder = st.text_area(\"Placeholder for the Elasticsearch response:\", value=default_response_instructions)\n",
        "            prompt_negative_response = st.text_area(\"Negative response placeholder:\", value=default_negative_response)\n",
        "            prompt_closing = st.text_area(\"Closing remarks of the prompt:\",\n",
        "                                          value=\"Answer the question in markdown bulletpoints.\")\n",
        "\n",
        "            combined_prompt = f\"{prompt_intro}\\n{prompt_query_placeholder}\\n{prompt_response_placeholder}\\n{prompt_negative_response}\\n{prompt_closing}\"\n",
        "            st.text_area(\"Preview of your custom prompt:\", value=combined_prompt, disabled=True)\n",
        "\n",
        "    with opt_col2:\n",
        "        with st.expander(\"Retrieval Search and Display Options\"):\n",
        "            st.subheader(\"Retrieval Options\")\n",
        "            ret_1, ret_2 = st.columns(2)\n",
        "            with ret_1:\n",
        "                search_method = st.radio(\"Search Method\", (\"Semantic Search\", \"Lexical Search\"))\n",
        "                augment_method = st.radio(\"Augment Method\", (\"Full Text\", \"Matching Chunk\"))\n",
        "            with ret_2:\n",
        "                # TODO this should update the title based on the augment_method\n",
        "                doc_count_title = \"Number of docs or chunks to Augment with\" if augment_method == \"Full Text\" else \"Number of Matching Chunks to Retrieve\"\n",
        "                doc_count = st.slider(doc_count_title, min_value=1, max_value=5, value=1)\n",
        "\n",
        "            st.subheader(\"Display Options\")\n",
        "            show_full_prompt = st.checkbox('Show Full Prompt Sent to LLM')\n",
        "\n",
        "    col1, col2 = st.columns(2)\n",
        "    with col1:\n",
        "        answer_button = st.form_submit_button(\"Find my answer!\")\n",
        "\n",
        "if answer_button:\n",
        "    search_method = \"knn\" if search_method == \"Semantic Search\" else \"bm25\"\n",
        "\n",
        "    apmclient.begin_transaction(\"query\")\n",
        "    elasticapm.label(search_method=search_method)\n",
        "    elasticapm.label(query=query)\n",
        "\n",
        "    try:\n",
        "        # Use combined_prompt and show_full_prompt as arguments\n",
        "        generate_response(query, es, search_method, combined_prompt, prompt_negative_response, show_full_prompt,\n",
        "                          doc_count, augment_method)\n",
        "        apmclient.end_transaction(\"query\", \"success\")\n",
        "    except Exception as e:\n",
        "        st.error(f\"An error occurred: {str(e)}\")\n",
        "        apmclient.end_transaction(\"query\", \"failure\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkZrWNJecO7r"
      },
      "source": [
        "## Step 2: Install localtunnel\n",
        "This is a Colab-specific requirement to allow us to connect to a Streamlit app we're building from within Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02NstyujMogn"
      },
      "outputs": [],
      "source": [
        "!npm install localtunnel --loglevel=error"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: let's get some images to make things pretty"
      ],
      "metadata": {
        "id": "B_4_BjsAb1NB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone \"https://github.com/elastic/genai-workshop-colab.git\"\n",
        "! cd genai-workshop-colab;  git checkout wave2; cd ..; cp -r ./genai-workshop-colab/notebooks/images images"
      ],
      "metadata": {
        "id": "nufMOSMbZ5Tr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wu0KfS0ESf6e"
      },
      "source": [
        "## Step 4: run Streamlit\n",
        "Running this cell will start local tunnel and generate a random URL\n",
        "\n",
        "Copy the IP address on the first line then open the generated URL and paste it in the input box \"Endpoint IP\"\n",
        "\n",
        "This will then start the Streamlit app"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHIHFID3NBXa"
      },
      "outputs": [],
      "source": [
        "!streamlit run app.py &>/content/logs.txt & npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}