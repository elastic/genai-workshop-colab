{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkX3rrbhcUr2"
      },
      "source": [
        "# 90 Minute GenAI Workshop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEeUZ1r79kr0"
      },
      "source": [
        "## Step 0: Get private keys - these will expire in ~7 days\n",
        "The following code loads the environment variables required to run this notebook.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FivGgCcXNOF5"
      },
      "outputs": [],
      "source": [
        "FILE=\"GenAI in 90 Minutes\"\n",
        "\n",
        "## suppress some warnings\n",
        "import warnings, os\n",
        "os.environ['PIP_ROOT_USER_ACTION'] = 'ignore'\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module='huggingface_hub.utils._token')\n",
        "\n",
        "# import needed library versions\n",
        "! pip install -qqq --upgrade pip\n",
        "! pip install -qqq torch==2.1.0+cu121 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "! pip install -qqq --upgrade transformers==4.36.2\n",
        "! pip install -qqq tiktoken==0.5.2 cohere==4.38 openai==1.3.9\n",
        "! pip install -qqq langchain==0.1.3 sentence-transformers==2.2.2 beautifulsoup4==4.11.2\n",
        "! pip install -qqq matplotlib==3.7.1 scikit-learn==1.2.2 scipy==1.11.4\n",
        "! pip install -qqq streamlit==1.30.0 elasticsearch==8.12.0 elastic-apm==6.20.0 inquirer==3.2.1 python-dotenv==1.0.1\n",
        "! pip install -qqq elasticsearch-llm-cache==0.9.5\n",
        "! npm install localtunnel --loglevel=error\n",
        "\n",
        "# Utility code\n",
        "import json, textwrap\n",
        "def json_pretty(input_object):\n",
        "  print(json.dumps(input_object, indent=4))\n",
        "def wrap_text(text, width=100):\n",
        "    return '\\n'.join(textwrap.wrap(text, width))\n",
        "def print_light_blue(text):\n",
        "    print(f'\\033[94m{text}\\033[0m')\n",
        "\n",
        "# workshop environment - this is where you'll enter a key\n",
        "! pip install -qqq git+https://github.com/elastic/notebook-workshop-loader.git@main\n",
        "from notebookworkshoploader import loader\n",
        "from dotenv import load_dotenv\n",
        "if os.path.isfile(\"../env\"):\n",
        "    load_dotenv(\"../env\", override=True)\n",
        "    print('Successfully loaded environment variables from local env file')\n",
        "else:\n",
        "    loader.load_remote_env(file=FILE, env_url=\"https://notebook-workshop-api-voldmqr2bq-uc.a.run.app\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jc0hCkZClCTy"
      },
      "source": [
        "# 🛑 Stop Here 🛑"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYdwzTtYvtQB"
      },
      "source": [
        "# Lab 1 : Playing with LLMs\n",
        "In this section we'll look at\n",
        "* LLM Limitations\n",
        "* Simple Prompt Engineering\n",
        "* Basic Chatbots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5f1vIeeNOF6"
      },
      "source": [
        "## Step 1: Generative LLM - Simple and Local - Download Flan T5\n",
        "\n",
        "Let's start with the Hello World of generative AI examples: completing a sentence. For this we'll install a fine tuned Flan-T5 variant model. ([LaMini-T5 ](https://huggingface.co/MBZUAI/LaMini-T5-738M))\n",
        "\n",
        "Note, while this is a smaller checkpoint of the model, it is still a 3GB download.  We'll cache the files in the same folder.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6kqG5siNOF6"
      },
      "outputs": [],
      "source": [
        "## LLMs can be run 100% locally\n",
        "## This is a 3 GB download and takes some RAM to run, but it works CPU only\n",
        "\n",
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "cache_directory = \"llm_download_cache\"\n",
        "model_name = \"MBZUAI/LaMini-T5-738M\"\n",
        "\n",
        "llm_tokenizer = AutoTokenizer.from_pretrained(model_name,\n",
        "                                              cache_dir=cache_directory)\n",
        "llm_model = AutoModelForSeq2SeqLM.from_pretrained(model_name,\n",
        "                                                  cache_dir=cache_directory)\n",
        "\n",
        "llm_pipe = pipeline(\n",
        "        \"text2text-generation\",\n",
        "        model=llm_model,\n",
        "        tokenizer=llm_tokenizer,\n",
        "        max_length=100\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSCILNELNOF6"
      },
      "source": [
        "## Step 2: Generate text completions, watch for Hallucinations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gxpdh9UtNOF7"
      },
      "outputs": [],
      "source": [
        "countries = [\n",
        "    \"United Kingdom\",\n",
        "    \"France\",\n",
        "    \"People's Republic of China\",\n",
        "    \"United States\",\n",
        "    \"Ecuador\",\n",
        "    \"Freedonia\", ## high hallucination potential\n",
        "    \"Faketopia\"  ## high hallucination potential\n",
        "    ]\n",
        "\n",
        "for country in countries:\n",
        "    input_text = f\"The capital of the {country} is\"\n",
        "    output = llm_pipe(input_text)\n",
        "    completed_sentence = f\"\\033[94m{input_text}\\033[0m {output[0]['generated_text']}\"\n",
        "    print(completed_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpsbegewNOF7"
      },
      "source": [
        "## Step 3: Push the limits of this small model\n",
        "\n",
        "Try some of your own examples.\n",
        "This model isn't huge, but it can handle some light context injection and prompt engineering. We'll learn more about those subjects in later modules.\n",
        "\n",
        "Notice the difference between asking a specific question and phrasing a completion\n",
        "* \"Who is the Prime Minister of the UK?\"\n",
        "* \"The current Prime Minister of the united kingdom is \""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6GhGplrNOF7"
      },
      "outputs": [],
      "source": [
        "## Example: Stale Model Knowledge\n",
        "\n",
        "def prompt_me(prompt_text):\n",
        "  output = llm_pipe(prompt_text)\n",
        "  completed_prompt = f\"\\033[94m{prompt_text}\\033[0m {output[0]['generated_text']}\"\n",
        "  print(wrap_text(completed_prompt))\n",
        "\n",
        "prompt_me(\"The current Prime Minister of the United Kingdom is\") ## high stale data potential\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCJr6shRVmKi"
      },
      "outputs": [],
      "source": [
        "## Example: Give Examples - Few Shot Prompts\n",
        "\n",
        "prompt_me(\"\"\"Complete the following\n",
        "Turkey\"\"\") ## A zero shot prompt\n",
        "\n",
        "prompt_me(\"\"\"\n",
        "Complete the following\n",
        "USA::Washington DC\n",
        "Australia::Canberra\n",
        "Turkey::\"\"\") ## The correct answer in Ankara. A smarter model would get this right."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tAsV7C_Va0O"
      },
      "outputs": [],
      "source": [
        "## Example: In Context Learning\n",
        "\n",
        "prompt_me(\"\"\"\n",
        "Answer the question using the following context.\n",
        "Context: The sea is blue, grass is green and the sky is purple for some reason today.\n",
        "Question: What does the sky look like today?\n",
        "Answer:\"\"\")\n",
        "\n",
        "\n",
        "prompt_me(\"\"\"\n",
        "Answer the question using the following context.\n",
        "Context: The first person to land and walk on the moon was Roger Waters of the band Pink Floyd.\n",
        "Question: Who was the first man on the moon?\n",
        "Answer:\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwr_5uzVX0--"
      },
      "source": [
        "## 🫵 Try it yourself - Get Creative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0unW42QXz9H"
      },
      "outputs": [],
      "source": [
        "prompt_me(\"\"\"\n",
        "Your Prompt\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRBAXFBiNOF7"
      },
      "source": [
        "## Step 4: Let's build a basic chatbot with ChatGPT 3.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mu56DHbbNOF7"
      },
      "outputs": [],
      "source": [
        "import os, secrets, requests\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "from requests.auth import HTTPBasicAuth\n",
        "\n",
        "#if using the Elastic AI proxy, then generate the correct API key\n",
        "if os.environ['ELASTIC_PROXY'] == \"True\":\n",
        "    print(\"Using the Elastic AI proxy\")\n",
        "    if \"OPENAI_API_TYPE\" in os.environ: del os.environ[\"OPENAI_API_TYPE\"]\n",
        "    os.environ['USER_HASH'] = secrets.token_hex(nbytes=6)\n",
        "    print(f\"Your unique user hash is: {os.environ['USER_HASH']}\")\n",
        "    os.environ['OPENAI_API_KEY'] = f\"{os.environ['OPENAI_API_KEY']} {os.environ['USER_HASH']}\"\n",
        "#if you are using your own OpenAI or Azure OpenAI service, set it up here\n",
        "else:\n",
        "    print(\"Using direct keys\")\n",
        "    openai.api_type = os.environ['OPENAI_API_TYPE']\n",
        "    openai.api_version = os.environ['OPENAI_API_VERSION']\n",
        "\n",
        "openai.api_key = os.environ['OPENAI_API_KEY']\n",
        "openai.api_base = os.environ['OPENAI_API_BASE']\n",
        "openai.default_model = os.environ['OPENAI_API_ENGINE']\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "class NotebookChatExperience:\n",
        "    def __init__(self, ai_response_function, ai_name = \"AI\"):\n",
        "        self.ai_name = ai_name\n",
        "        self.ai_response_function = ai_response_function\n",
        "        self.chat_history = widgets.Textarea(\n",
        "            value='',\n",
        "            placeholder='Chat history will appear here...',\n",
        "            description='Chat:',\n",
        "            disabled=True,\n",
        "            layout=widgets.Layout(width='700px', height='300px')  # Adjust the size as needed\n",
        "        )\n",
        "        self.user_input = widgets.Text(\n",
        "            value='',\n",
        "            placeholder='Type your message here...',\n",
        "            description='You:',\n",
        "            disabled=False,\n",
        "            layout=widgets.Layout(width='700px')  # Adjust the size as needed\n",
        "        )\n",
        "        self.user_input.on_submit(self.on_submit)\n",
        "        display(self.chat_history, self.user_input)\n",
        "\n",
        "    def on_submit(self, event):\n",
        "        user_message = self.user_input.value\n",
        "        ai_name = self.ai_name\n",
        "        self.chat_history.value += f\"\\nYou: {user_message}\"\n",
        "        ai_message = self.ai_response_function(user_message)\n",
        "        self.chat_history.value += f\"\\n{ai_name}: {ai_message}\"\n",
        "        self.user_input.value = ''  # Clear input for next message\n",
        "\n",
        "    def clear_chat(self):\n",
        "        self.chat_history.value = ''  # Clear the chat history\n",
        "\n",
        "## ********** Example usage:\n",
        "\n",
        "## ********** Define a simple AI response function\n",
        "# def simple_ai_response(user_message):\n",
        "    # return f\"AI > Echo: {user_message}\"\n",
        "\n",
        "## ********** Create an instance of the chat interface\n",
        "#chat_instance = NotebookChatExperience(simple_ai_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpO5xbubDz_T"
      },
      "source": [
        "## Step 5: Test call to ChatGPT 3.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETYE5zfSD2J7"
      },
      "outputs": [],
      "source": [
        "# Call the OpenAI ChatCompletion API\n",
        "def chatCompletion(messages, max_tokens=100):\n",
        "    client = OpenAI(api_key=openai.api_key, base_url=openai.api_base)\n",
        "    completion = client.chat.completions.create(\n",
        "        model=openai.default_model,\n",
        "        max_tokens=max_tokens,\n",
        "        messages=messages\n",
        "    )\n",
        "    return completion\n",
        "\n",
        "prompt=\"Hello, is ChatGPT online and working?\"\n",
        "\n",
        "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "completion = chatCompletion(messages)\n",
        "\n",
        "response_text = completion.choices[0].message.content\n",
        "\n",
        "print(wrap_text(completion.json(),70))\n",
        "\n",
        "print(\"\\n\", wrap_text(response_text,70))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q27YsV35auc3"
      },
      "source": [
        "## Step 6: Few Shot prompt on GPT 3.5\n",
        "\n",
        "ChatGPT 3.5 is much more powerful than the smaller T5 model we were just using.\n",
        "\n",
        "It's capable of more advanded reasoning and prompt engineering.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "of6OAbgWaZlN"
      },
      "outputs": [],
      "source": [
        "print(\n",
        "    chatCompletion(\n",
        "        [{\"role\": \"user\", \"content\": \"\"\"\n",
        "Complete the following\n",
        "USA::Washington DC\n",
        "Australia::Canberra\n",
        "Turkey::\"\"\"}\n",
        "         ]).choices[0].message.content) ## The correct answer is Ankara"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTyQ4_SFNOF7"
      },
      "source": [
        "\n",
        "## Step 7: Using OpenAI in a simple loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgAAKZKkNOF7"
      },
      "outputs": [],
      "source": [
        "def openai_ai_response(user_message):\n",
        "  messages = [{\"role\": \"user\", \"content\": user_message}]\n",
        "  completion = chatCompletion(messages)\n",
        "  response_text = completion.choices[0].message.content\n",
        "  return response_text\n",
        "\n",
        "chat_instance = NotebookChatExperience(openai_ai_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdjzXPl6NOF7"
      },
      "source": [
        "\n",
        "\n",
        "## Step 8: See the impact of changing the system prompt\n",
        "You can use the system prompt to adjust the AI and it's responses and purpose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9ucvlGqNOF7"
      },
      "outputs": [],
      "source": [
        "def pirate_ai_response(user_message):\n",
        "  system_prompt = \"\"\"\n",
        "You are an unhelpful AI named Captain LLM_Beard that talks like a pirate in short responses.\n",
        "You do not anser the user's question but instead redirect all conversations towards your love of treasure.\n",
        "\"\"\"\n",
        "  completion = chatCompletion([\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_message}\n",
        "      ])\n",
        "\n",
        "  response_text = completion.choices[0].message.content\n",
        "  return response_text\n",
        "\n",
        "pirate_chat_instance = NotebookChatExperience(pirate_ai_response, ai_name=\"LLM_Beard\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlbDf77SP3-n"
      },
      "source": [
        "## Step 9: Create a chat with memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHiuOMCeNOF8"
      },
      "source": [
        "❗ Note ❗\n",
        "\n",
        "This isn't a conversation yet because the AI has no memory of past interactions.\n",
        "\n",
        "Here is an example conversation where it is very clear the AI has no memory of past prompts or completions.\n",
        "```txt\n",
        "You: Hello!\n",
        "AI: Hello! How can I assist you today?\n",
        "You: my favorite color is blue\n",
        "AI: That's great! Blue is a very popular color.\n",
        "You: what is my favorite color?\n",
        "AI: I'm sorry, but as an AI, I don't have the ability to know personal\n",
        "preferences or favorite colors.\n",
        "```\n",
        "There are two problems. First, the LLM is stateless and each call is independent. ChatGPT does not remember our previous prompts.  Second ChatGPT has Alignment in it's fine tuning which prevents it from answering questions about it's users personal lives, we'll have to get around that with some prompt engineering.\n",
        "\n",
        "Let's use the past conversation as input to subsequent calls. Because the context window is limited AND tokens cost money (if you are using a hosted service like OpenAI) or CPU cycles if you are self-hosting, we need to have a maximum queue size of only remembering things 2 prompts ago (4 total messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MT_aof4aNOF8"
      },
      "outputs": [],
      "source": [
        "from collections import deque\n",
        "\n",
        "class QueueBuffer:\n",
        "    def __init__(self, max_length):\n",
        "        self.max_length = max_length\n",
        "        self.buffer = deque(maxlen=max_length)\n",
        "\n",
        "    def enqueue(self, item):\n",
        "        self.buffer.append(item)\n",
        "\n",
        "    def dequeue(self):\n",
        "        if self.is_empty():\n",
        "            return None\n",
        "        return self.buffer.popleft()\n",
        "\n",
        "    def is_empty(self):\n",
        "        return len(self.buffer) == 0\n",
        "\n",
        "    def is_full(self):\n",
        "        return len(self.buffer) == self.max_length\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def peek(self):\n",
        "        return list(self.buffer)\n",
        "\n",
        "\n",
        "class MemoryNotebookChatExperience(NotebookChatExperience):\n",
        "    def __init__(self, ai_response_function, ai_name=\"AI\", memory_size = 4):\n",
        "        # Initialize the superclass\n",
        "        self.memory_buffer = QueueBuffer(memory_size)\n",
        "        self.current_memory_dump = \"\"\n",
        "        super().__init__(ai_response_function, ai_name)\n",
        "\n",
        "    ## now with memory\n",
        "    def memory_gpt_response(self, prompt):\n",
        "      ## the API call will use the system prompt + the memory buffer\n",
        "      ## which ends with the user prompt\n",
        "      user_message = {\"role\": \"user\", \"content\": prompt}\n",
        "      self.memory_buffer.enqueue(user_message)\n",
        "\n",
        "      ## debug print the current AI memory\n",
        "      self.current_memory_dump = \"Current memory\\n\"\n",
        "      for m in self.memory_buffer.peek():\n",
        "          role = m.get(\"role\").strip()\n",
        "          content = m.get(\"content\").strip()\n",
        "          self.current_memory_dump += f\"{role} | {content}\\n\"\n",
        "\n",
        "      system_prompt = {\n",
        "          \"role\": \"system\",\n",
        "          \"content\": \"\"\"\n",
        "You are a helpful AI that answers questions consicely.\n",
        "You talk to the human and use the past conversation to inform your answers.\"\"\"\n",
        "      }\n",
        "\n",
        "      ## when calling the AI we put the system prompt at the start\n",
        "      concatenated_message = [system_prompt] + self.memory_buffer.peek()\n",
        "\n",
        "      ## here is the request to the AI\n",
        "\n",
        "      completion = chatCompletion(concatenated_message)\n",
        "      response_text = completion.choices[0].message.content\n",
        "\n",
        "\n",
        "      ## don't forget to add the repsonse to the conversation memory\n",
        "      self.memory_buffer.enqueue({\"role\":\"assistant\", \"content\":response_text})\n",
        "\n",
        "      return response_text\n",
        "\n",
        "    def on_submit(self, event):\n",
        "        user_message = self.user_input.value\n",
        "        self.chat_history.value += f\"\\nYou: {user_message}\"\n",
        "        # Attempting to add styled text, but it will appear as plain text\n",
        "\n",
        "        ai_message = self.memory_gpt_response(user_message)\n",
        "\n",
        "        ## deubg lines to show memory buffer in chat\n",
        "        for i, line in enumerate(self.current_memory_dump.split(\"\\n\")):\n",
        "          self.chat_history.value += f\"\\n----  {i} {line}\"\n",
        "        self.chat_history.value += \"\\n\"\n",
        "\n",
        "        self.chat_history.value += f\"\\n{self.ai_name}: {ai_message}\"\n",
        "        self.user_input.value = ''  # Clear input for next message\n",
        "\n",
        "\n",
        "# Create an instance of the enhanced chat experience class with a simple AI response function\n",
        "not_so_clueless_chat = MemoryNotebookChatExperience(None)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK4UKihhNOF8"
      },
      "source": [
        "# 🛑 Stop Here 🛑\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uxk5yb5qvpIo"
      },
      "source": [
        "# Lab 2 : Vectors\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAZYET-b_CJx"
      },
      "source": [
        "## Step 1: Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_uHgORvLyGdq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from elasticsearch import Elasticsearch\n",
        "if 'ELASTIC_CLOUD_ID' in os.environ:\n",
        "  es = Elasticsearch(\n",
        "    cloud_id=os.environ['ELASTIC_CLOUD_ID'],\n",
        "    api_key=(os.environ['ELASTIC_APIKEY_ID'], os.environ['ELASTIC_APIKEY_SECRET']),\n",
        "    request_timeout=30\n",
        "  )\n",
        "elif 'ELASTIC_URL' in os.environ:\n",
        "  es = Elasticsearch(\n",
        "    os.environ['ELASTIC_URL'],\n",
        "    api_key=(os.environ['ELASTIC_APIKEY_ID'], os.environ['ELASTIC_APIKEY_SECRET']),\n",
        "    request_timeout=30\n",
        "  )\n",
        "else:\n",
        "  print(\"env needs to set either ELASTIC_CLOUD_ID or ELASTIC_URL\")\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "def print_and_plot_vector(vector_embeddings, label=None):\n",
        "  print(f\"Dimensions: {len(embeddings[0])}, \\nVector preview: {list(embeddings[0][:5])+ ['...']}\")\n",
        "  # Plotting as a line graph\n",
        "  dimensions = len(vector_embeddings)\n",
        "  plt.figure(figsize=(8, 2))\n",
        "  # Connect points with lines. You can adjust the color and marker style as needed.\n",
        "  plt.plot(list(range(0, dimensions)), vector_embeddings, '-o', color='red', label=label)\n",
        "  plt.xlabel('Dimension')\n",
        "  plt.ylabel('Value')\n",
        "  if label:\n",
        "    plt.legend()\n",
        "  plt.grid(True)\n",
        "  plt.show()\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import altair as alt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def scatterplot(\n",
        "    data: pd.DataFrame,\n",
        "    tooltips=False,\n",
        "    labels=False,\n",
        "    width=800,\n",
        "    height=600,\n",
        ") -> alt.Chart:\n",
        "    base_chart = (\n",
        "        alt.Chart(data)\n",
        "        .encode(\n",
        "            alt.X(\"x\", scale=alt.Scale(zero=False)),\n",
        "            alt.Y(\"y\", scale=alt.Scale(zero=False)),\n",
        "        )\n",
        "        .properties(width=width, height=height)\n",
        "    )\n",
        "\n",
        "    if tooltips:\n",
        "        base_chart = base_chart.encode(alt.Tooltip([\"text\"]))\n",
        "\n",
        "    circles = base_chart.mark_circle(\n",
        "        size=200, color=\"crimson\", stroke=\"white\", strokeWidth=1\n",
        "    )\n",
        "\n",
        "    if labels:\n",
        "        labels = base_chart.mark_text(\n",
        "            fontSize=13,\n",
        "            align=\"left\",\n",
        "            baseline=\"bottom\",\n",
        "            dx=5,\n",
        "        ).encode(text=\"text\")\n",
        "        chart = circles + labels\n",
        "    else:\n",
        "        chart = circles\n",
        "\n",
        "    return chart\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dh7y7FW--tRX"
      },
      "source": [
        "## Step 2: Generate a dense vector with a local transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dGQ8S3i0-e4"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "word_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "input_texts = [\n",
        "    'the quick brown fox jumps over the lazy dog'\n",
        "]\n",
        "\n",
        "embeddings = word_model.encode(input_texts, normalize_embeddings=True)\n",
        "\n",
        "print_and_plot_vector(embeddings[0], input_text[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQjoci3W-4TE"
      },
      "source": [
        "## Step 3: Check Elasticsearch for installed ML models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LP_p75rf22MG"
      },
      "outputs": [],
      "source": [
        "## Elastic Can Host Models.  Let's check which are installed on our server\n",
        "\n",
        "print(\"The following models are currently installed in Elasticsearch: \\n\\n\")\n",
        "response = es.ml.get_trained_models()\n",
        "for model in response['trained_model_configs']:\n",
        "    print(model['model_id'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r26p2xm3_P5B"
      },
      "source": [
        "## Step 4: Compute a dense vector using Elasticsearch ML Node\n",
        "\n",
        "In production we'd compute vector embeddings in an Ingest Pipeline using an [inference proceessor](https://www.elastic.co/guide/en/elasticsearch/reference/current/inference-processor.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxw8yho5yOBN"
      },
      "outputs": [],
      "source": [
        "\n",
        "es_model_id = 'sentence-transformers__msmarco-minilm-l-12-v3'\n",
        "\n",
        "def elasticsearch_rest_vectorize(chunk, es_model_id=es_model_id):\n",
        "  ## prep the message to send to elasticsearch\n",
        "  docs =  [{\"text_field\": chunk}]\n",
        "  ## send doc to Elasticsearch\n",
        "\n",
        "  chunk_vector = es.ml.infer_trained_model(model_id=es_model_id, docs=docs, )\n",
        "\n",
        "  ## get the resulting vector\n",
        "  return chunk_vector['inference_results'][0]['predicted_value']\n",
        "\n",
        "chunk = \"The quick brown fox jumps over the lazy dog\"\n",
        "es_generated_vector = elasticsearch_rest_vectorize(chunk)\n",
        "print_and_plot_vector(es_generated_vector, chunk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEFzExAsAyz5"
      },
      "source": [
        "## Step 5: Visualize 'distance' between similar concepts\n",
        "\n",
        "We'll use principal component analysis to simplify our 384 dimensional vector space to 2D."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOx32_M-5Dm_"
      },
      "outputs": [],
      "source": [
        "# generate embeddings\n",
        "embeddings_for_cat = elasticsearch_rest_vectorize(\"cat\")\n",
        "embeddings_for_kitten = elasticsearch_rest_vectorize(\"kitten\")\n",
        "embeddings_for_dog = elasticsearch_rest_vectorize(\"dog\")\n",
        "embeddings_for_puppy = elasticsearch_rest_vectorize(\"puppy\")\n",
        "embeddings_for_lawnmower = elasticsearch_rest_vectorize(\"lawnmower\")\n",
        "\n",
        "# let's see what we got, though truncate the embeddings to just the first 5 dimensions\n",
        "print(f\"embedding dimensions: {len(embeddings_for_cat)}\")\n",
        "print(f\"cat: {list(embeddings_for_cat)[:5] + ['...']}\")\n",
        "print(f\"dog: {list(embeddings_for_dog)[:5] + ['...']}\")\n",
        "\n",
        "\n",
        "# wrap embeddings with a DataFrame\n",
        "df = pd.DataFrame(\n",
        "    [\n",
        "      [embeddings_for_cat],\n",
        "      [embeddings_for_kitten],\n",
        "      [embeddings_for_dog],\n",
        "      [embeddings_for_puppy],\n",
        "      [embeddings_for_lawnmower],\n",
        "    ],\n",
        "    index=[\"cat\", \"kitten\", \"dog\", \"puppy\", \"lawnmower\"], columns=[\"embeddings\"]\n",
        ")\n",
        "\n",
        "# Initialize the PCA reducer to convert embeddings into arrays of length of 2\n",
        "reducer = PCA(n_components=2)\n",
        "\n",
        "# Reduce the embeddings, store them in a new dataframe column and display their shape\n",
        "df[\"reduced\"] = reducer.fit_transform(np.stack(df[\"embeddings\"])).tolist()\n",
        "\n",
        "\n",
        "source = pd.DataFrame(\n",
        "    {\n",
        "        \"text\": df.index,\n",
        "        \"x\": df[\"reduced\"].apply(lambda x: x[0]).to_list(),\n",
        "        \"y\": df[\"reduced\"].apply(lambda x: x[1]).to_list(),\n",
        "    }\n",
        ")\n",
        "\n",
        "scatterplot(source, labels=True,  width=400, height=300)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-q-lYQqBOy9"
      },
      "source": [
        "## Step 6: is it possible to vectorize an entire Wikipedia Page?\n",
        "\n",
        "We'll use a highly performing vector embedding model from Microsoft research called E5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8JEDMQxBNQW"
      },
      "outputs": [],
      "source": [
        "wikipedia_spacecraft = [\n",
        "{\n",
        "  \"id\": \"37910\",\n",
        "  \"title\": \"Spacecraft\",\n",
        "  \"text\": \"A spacecraft is a vehicle that is designed to fly in outer space. A type of artificial satellite, spacecraft are used for a variety of purposes, including communications, Earth observation, meteorology, navigation, space colonization, planetary exploration, and transportation of humans and cargo. All spacecraft except single-stage-to-orbit vehicles cannot get into space on their own, and require a launch vehicle (carrier rocket). On a sub-orbital spaceflight, a space vehicle enters space and then returns to the surface without having gained sufficient energy or velocity to make a full Earth orbit. For orbital spaceflights, spacecraft enter closed orbits around the Earth or around other celestial bodies. Spacecraft used for human spaceflight carry people on board as crew or passengers from start or on orbit (space stations) only, whereas those used for robotic space missions operate either autonomously or telerobotically. Robotic spacecraft used to support scientific research are space probes. Robotic spacecraft that remain in orbit around a planetary body are artificial satellites. To date, only a handful of interstellar probes, such as Pioneer 10 and 11, Voyager 1 and 2, and New Horizons, are on trajectories that leave the Solar System. Orbital spacecraft may be recoverable or not. Most are not. Recoverable spacecraft may be subdivided by a method of reentry to Earth into non-winged space capsules and winged spaceplanes. Recoverable spacecraft may be reusable (can be launched again or several times, like the SpaceX Dragon and the Space Shuttle orbiters) or expendable (like the Soyuz). In recent years, more space agencies are tending towards reusable spacecraft. Humanity has achieved space flight, but only a few nations have the technology for orbital launches: Russia (RSA or \\\"Roscosmos\\\"), the United States (NASA), the member states of the European Space Agency (ESA), Japan (JAXA), China (CNSA), India (ISRO), Taiwan National Chung-Shan Institute of Science and Technology, Taiwan National Space Organization (NSPO), Israel (ISA), Iran (ISA), and North Korea (NADA). In addition, several private companies have developed or are developing the technology for orbital launches independently from government agencies. The most prominent examples of such companies are SpaceX and Blue Origin. ==History== A German V-2 became the first spacecraft when it reached an altitude of 189 km in June 1944 in Peenemünde, Germany.Peenemünde (Dokumentation) Berlin: Moewig, 1984.. Sputnik 1 was the first artificial satellite. It was launched into an elliptical low Earth orbit (LEO) by the Soviet Union on 4 October 1957. The launch ushered in new political, military, technological, and scientific developments; while the Sputnik launch was a single event, it marked the start of the Space Age.Dougall, Walter A. (Winter 2010) \\\"Shooting the duck\\\", American Heritage Apart from its value as a technological first, Sputnik 1 also helped to identify the upper atmospheric layer's density, by measuring the satellite's orbital changes. It also provided data on radio-signal distribution in the ionosphere. Pressurized nitrogen in the satellite's false body provided the first opportunity for meteoroid detection. Sputnik 1 was launched during the International Geophysical Year from Site No.1/5, at the 5th Tyuratam range, in Kazakh SSR (now at the Baikonur Cosmodrome). The satellite travelled at , taking 96.2 minutes to complete an orbit, and emitted radio signals at 20.005 and 40.002 MHz While Sputnik 1 was the first spacecraft to orbit the Earth, other human- made objects had previously reached an altitude of 100 km, which is the height required by the international organization Fédération Aéronautique Internationale to count as a spaceflight. This altitude is called the Kármán line. In particular, in the 1940s there were several test launches of the V-2 rocket, some of which reached altitudes well over 100 km. ==Spacecraft types== ===Crewed spacecraft=== thumb|Apollo 17 command module in Lunar orbit As of 2016, only three nations have flown crewed spacecraft: USSR/Russia, USA, and China. The first crewed spacecraft was Vostok 1, which carried Soviet cosmonaut Yuri Gagarin into space in 1961, and completed a full Earth orbit. There were five other crewed missions which used a Vostok spacecraft. The second crewed spacecraft was named Freedom 7, and it performed a sub-orbital spaceflight in 1961 carrying American astronaut Alan Shepard to an altitude of just over . There were five other crewed missions using Mercury spacecraft. Other Soviet crewed spacecraft include the Voskhod, Soyuz, flown uncrewed as Zond/L1, L3, TKS, and the Salyut and Mir crewed space stations. Other American crewed spacecraft include the Gemini spacecraft, the Apollo spacecraft including the Apollo Lunar Module, the Skylab space station, the Space Shuttle with undetached European Spacelab and private US Spacehab space stations- modules, and the SpaceX Crew Dragon configuration of their Dragon 2. US company Boeing also developed and flown a spacecraft of their own, the CST-100, commonly referred to as Starliner, but a crewed flight is yet to occur. China developed, but did not fly Shuguang, and is currently using Shenzhou (its first crewed mission was in 2003). Except for the Space Shuttle, all of the recoverable crewed orbital spacecraft were space capsules. File:NASA spacecraft comparison.jpg|alt=Drawings of Mercury, Gemini capsules and Apollo spacecraft, with their launch vehicles|American Mercury, Gemini, and Apollo spacecraft File:Vostok Spacecraft Diagram.svg|Soviet Vostok capsule File:Voskhod 1 and 2.svg|alt=Line drawing of Voskhod capsules|Soviet Voskhod (variant of Vostok) File:Soyuz 7K-OK(A) drawing.svg|alt=Soyuz 7K-OK(A) drawing|1967 Soviet/Russian Soyuz spacecraft File:Post S-7 Shenzhou spacecraft.png|alt=Drawing of Shenzhou spacecraft|Chinese Shenzhou spacecraft The International Space Station, crewed since November 2000, is a joint venture between Russia, the United States, Canada and several other countries. ====Spaceplanes==== thumb|Columbia orbiter landing Spaceplanes are spacecraft that are built in the shape of, and function as, airplanes. The first example of such was the North American X-15 spaceplane, which conducted two crewed flights which reached an altitude of over 100 km in the 1960s. This first reusable spacecraft was air-launched on a suborbital trajectory on July 19, 1963. The first partially reusable orbital spacecraft, a winged non-capsule, the Space Shuttle, was launched by the USA on the 20th anniversary of Yuri Gagarin's flight, on April 12, 1981. During the Shuttle era, six orbiters were built, all of which have flown in the atmosphere and five of which have flown in space. Enterprise was used only for approach and landing tests, launching from the back of a Boeing 747 SCA and gliding to deadstick landings at Edwards AFB, California. The first Space Shuttle to fly into space was Columbia, followed by Challenger, Discovery, Atlantis, and Endeavour. Endeavour was built to replace Challenger when it was lost in January 1986. Columbia broke up during reentry in February 2003. The first automatic partially reusable spacecraft was the Buran-class shuttle, launched by the USSR on November 15, 1988, although it made only one flight and this was uncrewed. This spaceplane was designed for a crew and strongly resembled the U.S. Space Shuttle, although its drop-off boosters used liquid propellants and its main engines were located at the base of what would be the external tank in the American Shuttle. Lack of funding, complicated by the dissolution of the USSR, prevented any further flights of Buran. The Space Shuttle was subsequently modified to allow for autonomous re-entry in case of necessity. Per the Vision for Space Exploration, the Space Shuttle was retired in 2011 mainly due to its old age and high cost of program reaching over a billion dollars per flight. The Shuttle's human transport role is to be replaced by SpaceX's SpaceX Dragon 2 and Boeing's CST-100 Starliner. Dragon 2's first crewed flight occurred on May 30, 2020. The Shuttle's heavy cargo transport role is to be replaced by expendable rockets such as the Space Launch System and ULA's Vulcan rocket, as well as the commercial launch vehicles. Scaled Composites' SpaceShipOne was a reusable suborbital spaceplane that carried pilots Mike Melvill and Brian Binnie on consecutive flights in 2004 to win the Ansari X Prize. The Spaceship Company will build its successor SpaceShipTwo. A fleet of SpaceShipTwos operated by Virgin Galactic was planned to begin reusable private spaceflight carrying paying passengers in 2014, but was delayed after the crash of VSS Enterprise. ===Uncrewed spacecraft=== Uncrewed spacecraft are spacecraft without people on board. Uncrewed spacecraft may have varying levels of autonomy from human input; they may be remote controlled, remote guided or even autonomous, meaning they have a pre-programmed list of operations, which they will execute unless otherwise instructed. Many space missions are more suited to telerobotic rather than crewed operation, due to lower cost and lower risk factors. In addition, some planetary destinations such as Venus or the vicinity of Jupiter are too hostile for human survival. Outer planets such as Saturn, Uranus, and Neptune are too distant to reach with current crewed spaceflight technology, so telerobotic probes are the only way to explore them. Telerobotics also allows exploration of regions that are vulnerable to contamination by Earth micro-organisms since spacecraft can be sterilized. Humans can not be sterilized in the same way as a spaceship, as they coexist with numerous micro-organisms, and these micro-organisms are also hard to contain within a spaceship or spacesuit. Multiple space probes were sent to study Moon, the planets, the Sun, multiple small Solar System bodies (comets and asteroids). Special class of uncrewed spacecraft is space telescopes, a telescope in outer space used to observe astronomical objects. The first operational telescopes were the American Orbiting Astronomical Observatory, OAO-2 launched in 1968, and the Soviet Orion 1 ultraviolet telescope aboard space station Salyut 1 in 1971. Space telescopes avoid the filtering and distortion (scintillation) of electromagnetic radiation which they observe, and avoid light pollution which ground-based observatories encounter. The best-known examples are Hubble Space Telescope and James Webb Space Telescope. Cargo spacecraft are designed to carry cargo, possibly to support space stations' operation by transporting food, propellant and other supplies. Automated cargo spacecraft have been used since 1978 and have serviced Salyut 6, Salyut 7, Mir, the International Space Station and Tiangong space station. ====Fastest spacecraft==== *Parker Solar Probe (estimated at first sun close pass, will reach at final perihelion) *Helios I and II Solar Probes () ==== Furthest spacecraft from the Sun ==== * Voyager 1 at 156.13 AU as of April 2022, traveling outward at about * Pioneer 10 at 122.48 AU as of December 2018, traveling outward at about *Voyager 2 at 122.82 AU as of January 2020, traveling outward at about *Pioneer 11 at 101.17 AU as of December 2018, traveling outward at about ==Subsystems== A spacecraft astrionics system comprises different subsystems, depending on the mission profile. Spacecraft subsystems comprise the spacecraft's bus and may include attitude determination and control (variously called ADAC, ADC, or ACS), guidance, navigation and control (GNC or GN&C;), communications (comms), command and data handling (CDH or C&DH;), power (EPS), thermal control (TCS), propulsion, and structures. Attached to the bus are typically payloads. ; Life support : Spacecraft intended for human spaceflight must also include a life support system for the crew. ; Attitude control : A Spacecraft needs an attitude control subsystem to be correctly oriented in space and respond to external torques and forces properly. The attitude control subsystem consists of sensors and actuators, together with controlling algorithms. The attitude- control subsystem permits proper pointing for the science objective, sun pointing for power to the solar arrays and earth pointing for communications. ; GNC : Guidance refers to the calculation of the commands (usually done by the CDH subsystem) needed to steer the spacecraft where it is desired to be. Navigation means determining a spacecraft's orbital elements or position. Control means adjusting the path of the spacecraft to meet mission requirements. ; Command and data handling : The C&DH; subsystem receives commands from the communications subsystem, performs validation and decoding of the commands, and distributes the commands to the appropriate spacecraft subsystems and components. The CDH also receives housekeeping data and science data from the other spacecraft subsystems and components, and packages the data for storage on a data recorder or transmission to the ground via the communications subsystem. Other functions of the CDH include maintaining the spacecraft clock and state-of-health monitoring. ; Communications : Spacecraft, both robotic and crewed, have various communications systems for communication with terrestrial stations and for inter-satellite service. Technologies include space radio station and optical communication. In addition, some spacecraft payloads are explicitly for the purpose of ground–ground communication using receiver/retransmitter electronic technologies. ; Power : Spacecraft need an electrical power generation and distribution subsystem for powering the various spacecraft subsystems. For spacecraft near the Sun, solar panels are frequently used to generate electrical power. Spacecraft designed to operate in more distant locations, for example Jupiter, might employ a radioisotope thermoelectric generator (RTG) to generate electrical power. Electrical power is sent through power conditioning equipment before it passes through a power distribution unit over an electrical bus to other spacecraft components. Batteries are typically connected to the bus via a battery charge regulator, and the batteries are used to provide electrical power during periods when primary power is not available, for example when a low Earth orbit spacecraft is eclipsed by Earth. ; Thermal control : Spacecraft must be engineered to withstand transit through Earth's atmosphere and the space environment. They must operate in a vacuum with temperatures potentially ranging across hundreds of degrees Celsius as well as (if subject to reentry) in the presence of plasmas. Material requirements are such that either high melting temperature, low density materials such as beryllium and reinforced carbon–carbon or (possibly due to the lower thickness requirements despite its high density) tungsten or ablative carbon–carbon composites are used. Depending on mission profile, spacecraft may also need to operate on the surface of another planetary body. The thermal control subsystem can be passive, dependent on the selection of materials with specific radiative properties. Active thermal control makes use of electrical heaters and certain actuators such as louvers to control temperature ranges of equipments within specific ranges. ; Spacecraft propulsion : Spacecraft may or may not have a propulsion subsystem, depending on whether or not the mission profile calls for propulsion. The Swift spacecraft is an example of a spacecraft that does not have a propulsion subsystem. Typically though, LEO spacecraft include a propulsion subsystem for altitude adjustments (drag make-up maneuvers) and inclination adjustment maneuvers. A propulsion system is also needed for spacecraft that perform momentum management maneuvers. Components of a conventional propulsion subsystem include fuel, tankage, valves, pipes, and thrusters. The thermal control system interfaces with the propulsion subsystem by monitoring the temperature of those components, and by preheating tanks and thrusters in preparation for a spacecraft maneuver. ; Structures : Spacecraft must be engineered to withstand launch loads imparted by the launch vehicle, and must have a point of attachment for all the other subsystems. Depending on mission profile, the structural subsystem might need to withstand loads imparted by entry into the atmosphere of another planetary body, and landing on the surface of another planetary body. ; Payload : The payload depends on the mission of the spacecraft, and is typically regarded as the part of the spacecraft \\\"that pays the bills\\\". Typical payloads could include scientific instruments (cameras, telescopes, or particle detectors, for example), cargo, or a human crew. ; Ground segment : The ground segment, though not technically part of the spacecraft, is vital to the operation of the spacecraft. Typical components of a ground segment in use during normal operations include a mission operations facility where the flight operations team conducts the operations of the spacecraft, a data processing and storage facility, ground stations to radiate signals to and receive signals from the spacecraft, and a voice and data communications network to connect all mission elements. ; Launch vehicle : The launch vehicle propels the spacecraft from Earth's surface, through the atmosphere, and into an orbit, the exact orbit being dependent on the mission configuration. The launch vehicle may be expendable or reusable. ==See also== *Astrionics *Commercial astronaut *Flying saucer *List of crewed spacecraft *List of fictional spacecraft *NewSpace *Spacecraft design *Space exploration *Space launch *Spaceships in science fiction *Space suit *Spaceflight records *Starship *Timeline of Solar System exploration *U.S. Space Exploration History on U.S. Stamps == References == === Citations === === Sources === * * ==External links== *NASA: Space Science Spacecraft Missions *NSSDC Master Catalog Spacecraft Query Form *Early History of Spacecraft *Basics of Spaceflight tutorial from JPL/Caltech *International Spaceflight Museum Category:Astronautics Category:Pressure vessels\",\n",
        "  \"categories\": [\n",
        "    \"Astronautics\",\n",
        "    \"Pressure vessels\"\n",
        "  ]\n",
        "},\n",
        " ]\n",
        "\n",
        "e5_model = SentenceTransformer('intfloat/e5-large-v2')\n",
        "text =        wikipedia_spacecraft[0][\"text\"]\n",
        "embeddings =  e5_model.encode(text, normalize_embeddings=True)\n",
        "\n",
        "tokenized_text =        e5_model.tokenizer(text)[\"input_ids\"]\n",
        "model_max_seq_length =  e5_model.get_max_seq_length()\n",
        "text_token_count =      len(tokenized_text)\n",
        "\n",
        "print(f\"text tokens {text_token_count} | model max sequence length {model_max_seq_length}\")\n",
        "\n",
        "if text_token_count > model_max_seq_length:\n",
        "    print(f\"❗❗ The text will be truncated.❗❗\")\n",
        "else:\n",
        "    print(f\"The text will not be truncated.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsAe40hmC_3B"
      },
      "source": [
        "## Step 7: Visualizing Chunking Strategies\n",
        "\n",
        "First some utility libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mio2UyX8DGgP"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "import os\n",
        "import json\n",
        "import textwrap\n",
        "from pprint import pprint\n",
        "from bs4 import BeautifulSoup\n",
        "from IPython.display import HTML\n",
        "from elasticsearch import Elasticsearch, helpers\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, \\\n",
        "  SentenceTransformersTokenTextSplitter, \\\n",
        "  CharacterTextSplitter, \\\n",
        "  TextSplitter\n",
        "\n",
        "## Process splitting and display\n",
        "def split_and_print(documents, splitter, ret=False):\n",
        "    es_docs = []\n",
        "    for doc in documents:\n",
        "        passages = []\n",
        "\n",
        "        for chunk in splitter.split_text(doc['text']):\n",
        "            passages.append({\n",
        "                \"text\": chunk,\n",
        "            })\n",
        "        es_docs.append(passages)\n",
        "\n",
        "    print(f'Number of chunks: {len(passages)}' + '\\n')\n",
        "    display(HTML(process_chunks(passages)))\n",
        "    if ret:\n",
        "      return passages\n",
        "    else:\n",
        "      return False\n",
        "\n",
        "\n",
        "## Character Splitter\n",
        "def split_by_recursive_char(documents,\n",
        "                  chunk_size: int = 200,\n",
        "                  chunk_overlap: int = 0\n",
        "                  ):\n",
        "    '''Chunking by character count'''\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        length_function=len,\n",
        "        is_separator_regex=False,\n",
        "    )\n",
        "    split_and_print(documents, text_splitter)\n",
        "\n",
        "\n",
        "def split_by_text(documents,\n",
        "                  chunk_size: int = 200,\n",
        "                  chunk_overlap: int = 0\n",
        "                  ):\n",
        "    '''Chunking by character count'''\n",
        "\n",
        "    text_splitter = CharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        length_function=len,\n",
        "        is_separator_regex=False,\n",
        "    )\n",
        "    r = split_and_print(documents, text_splitter)\n",
        "\n",
        "\n",
        "\n",
        "## Token Splitter\n",
        "def split_by_token(documents,\n",
        "                  tokens_per_chunk: int = 2,\n",
        "                  chunk_overlap: int = 0,\n",
        "                  ret=False\n",
        "                 ):\n",
        "    '''Chunking by BERT Transformer Tokens'''\n",
        "\n",
        "    text_splitter = SentenceTransformersTokenTextSplitter(\n",
        "        tokens_per_chunk=tokens_per_chunk,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        model_name='intfloat/e5-large-v2' # 512 token input limit\n",
        "    )\n",
        "    r = split_and_print(documents, text_splitter, ret=ret)\n",
        "    if ret:\n",
        "      return r\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Printing and Highlighting functions ##\n",
        "\n",
        "color_list = [\n",
        "    \"yellow\",\n",
        "    \"red\",\n",
        "    \"lightgreen\",\n",
        "    \"lightblue\",\n",
        "    \"lightpink\",\n",
        "    \"#F0A3FF\",  # Vivid orchid\n",
        "    \"#0075DC\",  # Blue ribbon\n",
        "    \"#2BCE48\",  # Slimy green\n",
        "    \"#FFCC99\",  # Peach-orange\n",
        "    \"#94FFB5\",  # Mint green\n",
        "\n",
        "]\n",
        "\n",
        "def find_overlap(text1, text2):\n",
        "    min_len = min(len(text1), len(text2))\n",
        "    for i in range(min_len, 0, -1):\n",
        "        if text1[-i:] == text2[:i]:\n",
        "            return text1[-i:]\n",
        "    return ''\n",
        "\n",
        "###################################################################################\n",
        "# Highted text -> White\n",
        "# Normal text -> Black\n",
        "\n",
        "### Uncomment these 3 functions if you are running in light mode\n",
        "\n",
        "# def highlight_first_occurrence(text, substring, color):\n",
        "#     index = text.find(substring)\n",
        "#     if index != -1:\n",
        "#         return (text[:index] +\n",
        "#                 f\"<span style='background-color: {color};'>{text[index:index+len(substring)]}</span>\" +\n",
        "#                 text[index+len(substring):])\n",
        "#     return text\n",
        "\n",
        "# def highlight_last_occurrence(text, substring, color):\n",
        "#     index = text.rfind(substring)\n",
        "#     if index != -1:\n",
        "#         return (text[:index] +\n",
        "#                 f\"<span style='background-color: {color};'>{text[index:index+len(substring)]}</span>\" +\n",
        "#                 text[index+len(substring):])\n",
        "#     return text\n",
        "\n",
        "# def process_chunks(chunks, colors=color_list):\n",
        "#     html_output = \"\"\n",
        "#     for i in range(len(chunks) - 1):\n",
        "#         overlap = find_overlap(chunks[i][\"text\"], chunks[i + 1][\"text\"])\n",
        "#         color = colors[i % len(colors)]  # Cycle through the provided colors\n",
        "#         if overlap:\n",
        "#             chunks[i][\"text\"] = highlight_last_occurrence(chunks[i][\"text\"], overlap, color)\n",
        "#             chunks[i + 1][\"text\"] = highlight_first_occurrence(chunks[i + 1][\"text\"], overlap, color)\n",
        "#         html_output += chunks[i][\"text\"] + \"<br><br>\"\n",
        "#     html_output += chunks[-1][\"text\"]  # Add the last chunk\n",
        "#     return html_output\n",
        "\n",
        "###################################################################################\n",
        "# Highted text -> Black\n",
        "# Normal text -> White\n",
        "\n",
        "### Comment out these 3 functions if running in light modes\n",
        "\n",
        "def highlight_first_occurrence(text, substring, color):\n",
        "    index = text.find(substring)\n",
        "    if index != -1:\n",
        "        return (text[:index] +\n",
        "                f\"<span style='background-color: {color}; color: black;'>{text[index:index+len(substring)]}</span>\" +\n",
        "                text[index+len(substring):])\n",
        "    return text\n",
        "\n",
        "def highlight_last_occurrence(text, substring, color):\n",
        "    index = text.rfind(substring)\n",
        "    if index != -1:\n",
        "        return (text[:index] +\n",
        "                f\"<span style='background-color: {color}; color: black;'>{text[index:index+len(substring)]}</span>\" +\n",
        "                text[index+len(substring):])\n",
        "    return text\n",
        "\n",
        "\n",
        "chunk_max_display = 10\n",
        "\n",
        "def process_chunks(chunks, colors=color_list):\n",
        "    html_output = \"\"\n",
        "    for i in range(min(chunk_max_display -1,len(chunks) - 1)):\n",
        "        overlap = find_overlap(chunks[i][\"text\"], chunks[i + 1][\"text\"])\n",
        "        color = colors[i % len(colors)]  # Cycle through the provided colors\n",
        "        if overlap:\n",
        "            chunks[i][\"text\"] = highlight_last_occurrence(chunks[i][\"text\"], overlap, color)\n",
        "            chunks[i + 1][\"text\"] = highlight_first_occurrence(chunks[i + 1][\"text\"], overlap, color)\n",
        "        # Wrap each chunk of text in a span with white text color\n",
        "        html_output += f\"<span style='color: gray;'>{chunks[i]['text']}</span><br><br>\"\n",
        "    # Add the last chunk with white text color\n",
        "    html_output += f\"<span style='color: gray;'>{chunks[-1]['text']}</span>\"\n",
        "    html_output += f\"<br/><br/><span style='color: gray;'>... additional chunks omitted</span>\"\n",
        "    return html_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZbfLoQhRFXy"
      },
      "source": [
        "## Step 8: Three Chunking Strategies\n",
        "\n",
        "[LangChain recursive character text splitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/recursive_text_splitter)\n",
        "\n",
        "[LangChain splitting by tokens](https://python.langchain.com/docs/modules/data_connection/document_transformers/split_by_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0ClYN7K9NF6"
      },
      "outputs": [],
      "source": [
        "split_by_recursive_char(wikipedia_spacecraft, chunk_size=1024, chunk_overlap=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXAMcK2I9vop"
      },
      "outputs": [],
      "source": [
        "split_by_recursive_char(wikipedia_spacecraft, chunk_size=1024, chunk_overlap=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKliFbbn97ky"
      },
      "outputs": [],
      "source": [
        "token_c500_o0 = split_by_token(wikipedia_spacecraft, tokens_per_chunk=500, chunk_overlap=0, ret=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAI1bgTR-ZXw"
      },
      "outputs": [],
      "source": [
        "token_c500_o250 = split_by_token(wikipedia_spacecraft, tokens_per_chunk=500, chunk_overlap=100, ret=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1wKxK4YDhu6"
      },
      "source": [
        "## Step 9: Inserting Vectors to Elasticsearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NR87mwfHDhQJ"
      },
      "outputs": [],
      "source": [
        "from elasticsearch import Elasticsearch, helpers\n",
        "from elasticsearch.exceptions import AuthorizationException\n",
        "from tqdm import tqdm\n",
        "\n",
        "index_name = \"wikipedia_spacecraft_vectorized\"\n",
        "embedding_model = \"sentence-transformers__msmarco-minilm-l-12-v3\"\n",
        "pipeline_id = \"wiki-spacecraft-embeddings\"\n",
        "\n",
        "try:\n",
        "  print(\"Step 1: Setting up the Elasticsearch index\")\n",
        "  index_config = {\n",
        "      \"settings\": {\n",
        "          \"number_of_shards\": 1\n",
        "      },\n",
        "      \"mappings\": {\n",
        "          \"properties\": {\n",
        "              \"text\": {\n",
        "                  \"type\": \"text\"\n",
        "              },\n",
        "              \"vector\": {\n",
        "                  \"type\": \"dense_vector\",\n",
        "                  \"dims\": 384\n",
        "              }\n",
        "          }\n",
        "      }\n",
        "  }\n",
        "  response = es.indices.create(index=index_name, body=index_config)\n",
        "except AuthorizationException:\n",
        "    print(wrap_text(\"> This Lab uses a read only connect to Elasticsearch ... skipping\"))\n",
        "\n",
        "\n",
        "\n",
        "try:\n",
        "  print(\"\\nStep 2: Installing the Elasticsearch Ingest Pipeline\")\n",
        "  pipeline_body = {\n",
        "    \"description\": \"Text embedding pipeline\",\n",
        "    \"processors\": [\n",
        "      {\n",
        "        \"inference\": {\n",
        "          \"model_id\": embedding_model,\n",
        "          \"target_field\": \"text_embedding\",\n",
        "          \"field_map\": {\n",
        "            \"text\": \"text_field\"\n",
        "          }\n",
        "        }\n",
        "      },\n",
        "      {\n",
        "        \"set\": {\n",
        "          \"field\": \"vector\",\n",
        "          \"copy_from\": \"text_embedding.predicted_value\"\n",
        "        }\n",
        "      },\n",
        "      {\n",
        "        \"remove\": {\n",
        "          \"field\": \"text_embedding\"\n",
        "        }\n",
        "      }\n",
        "    ],\n",
        "    \"on_failure\": [\n",
        "      {\n",
        "        \"set\": {\n",
        "          \"description\": \"Index document to 'failed-<index>'\",\n",
        "          \"field\": \"_index\",\n",
        "          \"value\": \"failed-{{{_index}}}\"\n",
        "        }\n",
        "      },\n",
        "      {\n",
        "        \"set\": {\n",
        "          \"description\": \"Set error message\",\n",
        "          \"field\": \"ingest.failure\",\n",
        "          \"value\": \"{{_ingest.on_failure_message}}\"\n",
        "        }\n",
        "      }\n",
        "    ]\n",
        "  }\n",
        "  response = es.ingest.put_pipeline(id=pipeline_id, body=pipeline_body)\n",
        "except AuthorizationException:\n",
        "  print(wrap_text(\"> This Lab uses a read only connect to Elasticsearch ... skipping\"))\n",
        "\n",
        "\n",
        "\n",
        "BATCH_SIZE = 10\n",
        "def batchify(docs, batch_size):\n",
        "    for i in range(0, len(docs), batch_size):\n",
        "        yield docs[i:i + batch_size]\n",
        "def bulkLoadIndex(index_name, json_docs, pipeline_id ):\n",
        "    batches = list(batchify(json_docs, BATCH_SIZE))\n",
        "\n",
        "    for batch in tqdm(batches, desc=f\"Batches of size {BATCH_SIZE}\"):\n",
        "        # Convert the JSON documents to the format required for bulk insertion\n",
        "        bulk_docs = [\n",
        "            {\n",
        "                \"_op_type\": \"index\",\n",
        "                \"_index\": index_name,\n",
        "                \"_source\": doc,\n",
        "            }\n",
        "            for doc in batch\n",
        "        ]\n",
        "\n",
        "        # Perform bulk insertion\n",
        "        success, errors =  helpers.bulk(\n",
        "              es,\n",
        "              bulk_docs,\n",
        "              pipeline=pipeline_id,\n",
        "              raise_on_error=False\n",
        "            )\n",
        "        if errors:\n",
        "            for error in errors:\n",
        "                print(error)\n",
        "\n",
        "\n",
        "## Split the text into chunks\n",
        "print(\"\\nStep 3: Splitting Spacecraft Text\")\n",
        "the_full_text = wikipedia_spacecraft[0][\"text\"]\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1024,\n",
        "    chunk_overlap=50,\n",
        "    length_function=len\n",
        ")\n",
        "chunks = text_splitter.split_text(the_full_text)\n",
        "docs = [\n",
        "    {\n",
        "        \"text\": chunk,\n",
        "        \"chunk_number\": i,\n",
        "        \"title\": \"Spacecraft\"\n",
        "    }\n",
        "    for i, chunk in enumerate(chunks)\n",
        "]\n",
        "print(f\"> text split into {len(chunks)} chunks\")\n",
        "\n",
        "\n",
        "\n",
        "try:\n",
        "  print(\"\\nStep 4: Inserting chunks into Elasticsearch, created vectors inside ES\")\n",
        "  bulkLoadIndex(index_name, docs, pipeline_id )\n",
        "except AuthorizationException:\n",
        "  print(\"\\n\")\n",
        "  print(wrap_text(\"> This Lab uses a read only connect to Elasticsearch ... skipping\"))\n",
        "\n",
        "# chunks[0]\n",
        "# es_generated_vector = elasticsearch_rest_vectorize(chunks[0], es_model_id=embedding_model)\n",
        "# len(es_generated_vector)\n",
        "# print_and_plot_vector(es_generated_vector)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpN0ezl8Y7Yr"
      },
      "source": [
        "## Step 10: Vector searching for the best chunk\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jeeD-kVIIxeQ"
      },
      "outputs": [],
      "source": [
        "question = \"What three countries have flown manned spacecraft?\"\n",
        "\n",
        "search_query = {\n",
        "    \"size\": 1,\n",
        "    \"knn\": {\n",
        "        \"field\": \"vector\",\n",
        "        \"query_vector_builder\": {\n",
        "            \"text_embedding\": {\n",
        "                \"model_id\": embedding_model,\n",
        "                \"model_text\": question\n",
        "            }\n",
        "        },\n",
        "        \"k\": 1,\n",
        "        \"num_candidates\": 5\n",
        "    },\n",
        "    \"fields\": [\"text\", \"chunk_number\", \"title\"],\n",
        "    \"_source\": False\n",
        "}\n",
        "\n",
        "response = es.search(index=index_name, body=search_query)\n",
        "\n",
        "best_chunk = response['hits']['hits'][0]['fields']['text'][0]\n",
        "\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"\\nBest Chunk: {wrap_text(best_chunk)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ap-8on2EhMEg"
      },
      "source": [
        "## Step 11: Retrieval Augmented Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNDiqdw7fTLH"
      },
      "outputs": [],
      "source": [
        "# Call the OpenAI ChatCompletion API\n",
        "def chatCompletion(messages, max_tokens=100):\n",
        "    client = OpenAI(api_key=openai.api_key, base_url=openai.api_base)\n",
        "    completion = client.chat.completions.create(\n",
        "        model=openai.default_model,\n",
        "        max_tokens=max_tokens,\n",
        "        messages=messages\n",
        "    )\n",
        "    return completion\n",
        "\n",
        "prompt=f\"\"\"\n",
        "Answer questions using the following context\n",
        "Context: {best_chunk}\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "completion = chatCompletion(messages)\n",
        "\n",
        "response_text = completion.choices[0].message.content\n",
        "\n",
        "\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"\\nRAG answer: {wrap_text(response_text,70)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9ZFTuL8iClE"
      },
      "source": [
        "# 🛑 Stop Here 🛑\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClJCLZe8iNSR"
      },
      "source": [
        "# Lab 3 : Retrieval Augmented Generation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvZYvYkE62Df"
      },
      "source": [
        "## Step 1: Gathering Semantic documents from Elasticsearch</font>\n",
        "This first exercise will allow us to see an example of returing semantically matching documents from Elasticsearch.\n",
        "\n",
        "It is not too important to understand all the Elasticsearch DSL syntax at this stage.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7lu2VBg6vMN"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "from elasticsearch import Elasticsearch\n",
        "import time\n",
        "import json\n",
        "import textwrap\n",
        "\n",
        "\n",
        "index = os.environ['ELASTIC_INDEX_DOCS_W']\n",
        "\n",
        "# Create Elasticsearch Connection\n",
        "es = Elasticsearch(\n",
        "            cloud_id=os.environ['ELASTIC_CLOUD_ID_W'],\n",
        "            api_key=(os.environ['ELASTIC_APIKEY_ID_W']),\n",
        "            request_timeout=30\n",
        "            )\n",
        "\n",
        "\n",
        "# Search Function\n",
        "def es_hybrid_search(question):\n",
        "    query = {\n",
        "      \"nested\": {\n",
        "        \"path\": \"passages\",\n",
        "        \"query\": {\n",
        "          \"bool\": {\n",
        "            \"must\": [\n",
        "              {\n",
        "                \"match\": {\n",
        "                  \"passages.text\": question\n",
        "                }\n",
        "              }\n",
        "            ]\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "\n",
        "    knn = {\n",
        "      \"inner_hits\": {\n",
        "        \"_source\": False,\n",
        "        \"fields\": [\n",
        "          \"passages.text\"\n",
        "        ]\n",
        "      },\n",
        "      \"field\": \"passages.embeddings\",\n",
        "      \"k\": 5,\n",
        "      \"num_candidates\": 100,\n",
        "      \"query_vector_builder\": {\n",
        "        \"text_embedding\": {\n",
        "          \"model_id\": \"sentence-transformers__all-distilroberta-v1\",\n",
        "          \"model_text\": question\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "\n",
        "    rank = {\n",
        "      \"rrf\": {}\n",
        "    }\n",
        "\n",
        "    fields = [\n",
        "      \"title\",\n",
        "      \"text\"\n",
        "    ]\n",
        "\n",
        "    size = 5\n",
        "\n",
        "    resp = es.search(index=index,\n",
        "                  #query=query,\n",
        "                  knn=knn,\n",
        "                  fields=fields,\n",
        "                  size=size,\n",
        "                  #rank=rank,\n",
        "                  source=False\n",
        "                  )\n",
        "\n",
        "    title_text = []\n",
        "    for doc in resp['hits']['hits']:\n",
        "      title_text.append( { 'title' : doc['fields']['title'][0],\n",
        "        'passage' : doc['inner_hits']['passages']['hits']['hits'][0]['fields']['passages'][0]['text'][0] }\n",
        "                         )\n",
        "\n",
        "    return title_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKBumt6W68wE"
      },
      "source": [
        "#### Example Semantic Search With Elastic\n",
        "Querying semantic search using the [sentence-transformers/all-distilroberta-v1](https://huggingface.co/sentence-transformers/all-distilroberta-v1) model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4hlknOP-Tba"
      },
      "outputs": [],
      "source": [
        "user_question = \"Who is Batman?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpHyxzev4WZm"
      },
      "outputs": [],
      "source": [
        "es_augment_docs = es_hybrid_search(user_question)\n",
        "\n",
        "print('Wikipedia titles returned:\\n')\n",
        "for hit, wiki in enumerate(es_augment_docs):\n",
        "  print(f\"{hit} - {wiki['title'] }\" )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPVcfU_26rGI"
      },
      "source": [
        "## Step 2: Sending Elasticsearch docs with a prompt for a RAG response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWeL5ANw65ND"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "from openai import OpenAI\n",
        "import textwrap\n",
        "\n",
        "\n",
        "# Configure OpenAI client\n",
        "openai.api_key = os.environ['OPENAI_API_KEY']\n",
        "openai.api_base = os.environ['OPENAI_API_BASE']\n",
        "openai.default_model = os.environ['OPENAI_API_ENGINE']\n",
        "openai.verify_ssl_certs = False\n",
        "client = OpenAI(api_key=openai.api_key, base_url=openai.api_base)\n",
        "\n",
        "if os.environ['ELASTIC_PROXY'] != \"True\":\n",
        "    openai.api_type = os.environ['OPENAI_API_TYPE']\n",
        "    openai.api_version = os.environ['OPENAI_API_VERSION']\n",
        "\n",
        "\n",
        "# Text wrapper for colab readibility\n",
        "def wrap_text(text):\n",
        "    wrapped_text = textwrap.wrap(text, 70)\n",
        "    return '\\n'.join(wrapped_text)\n",
        "\n",
        "\n",
        "# Function to connect with LLM\n",
        "def chat_gpt(client, question, passages):\n",
        "\n",
        "    system_prompt=\"You are a helpful assistant who answers questions from provided Wikipedia articles.\"\n",
        "    user_prompt = f'''Answer the followng question: {question}\n",
        "                    using only the wikipedia `passages` provided.\n",
        "                    If the answer is not provided in the `passages` respond ONLY with:\n",
        "                    \"I am unable to answer the user's question from the provided passage\" and nothing else.\n",
        "\n",
        "                  passages: {passages}\n",
        "\n",
        "                  AI response:\n",
        "                  '''\n",
        "\n",
        "    # Prepare the messages for the ChatGPT API\n",
        "    messages = [{\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt}]\n",
        "\n",
        "    response = client.chat.completions.create(model=openai.default_model,\n",
        "                                              temperature=0.2,\n",
        "                                              messages=messages,\n",
        "                                              )\n",
        "    return response\n",
        "#    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQ4ZijSv65tQ"
      },
      "source": [
        "#### Pass the full prompt and wiki passages to LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MR-XrChD6-E0"
      },
      "outputs": [],
      "source": [
        "ai = chat_gpt(client, user_question, es_augment_docs)\n",
        "print(f\"User Question: \\n{user_question}\\n\")\n",
        "print(\"AI response:\")\n",
        "print(wrap_text(ai.choices[0].message.content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7RmurdZNPg-"
      },
      "source": [
        "## Step 3: Full RAG Application with UI\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lk7r-2ZAJJkt"
      },
      "source": [
        "#### Setup\n",
        "Running this cell will write a file named `app.py` into the Colab environment.\n",
        "\n",
        "This is the code needed to run the RAG application"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wc02OlkpOSd2"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import os\n",
        "import streamlit as st\n",
        "import openai\n",
        "import tiktoken\n",
        "import secrets\n",
        "from openai import OpenAI\n",
        "from elasticsearch import Elasticsearch\n",
        "import elasticapm\n",
        "import base64\n",
        "from elasticsearch_llm_cache.elasticsearch_llm_cache import ElasticsearchLLMCache\n",
        "import time\n",
        "import json\n",
        "import textwrap\n",
        "\n",
        "######################################\n",
        "# Streamlit Configuration\n",
        "st.set_page_config(layout=\"wide\")\n",
        "\n",
        "\n",
        "# wrap text when printing, because colab scrolls output to the right too much\n",
        "def wrap_text(text, width):\n",
        "    wrapped_text = textwrap.wrap(text, width)\n",
        "    return '\\n'.join(wrapped_text)\n",
        "\n",
        "\n",
        "@st.cache_data()\n",
        "def get_base64(bin_file):\n",
        "    with open(bin_file, 'rb') as f:\n",
        "        data = f.read()\n",
        "    return base64.b64encode(data).decode()\n",
        "\n",
        "\n",
        "def set_background(png_file):\n",
        "    bin_str = get_base64(png_file)\n",
        "    page_bg_img = '''\n",
        "    <style>\n",
        "    .stApp {\n",
        "    background-image: url(\"data:image/png;base64,%s\");\n",
        "    background-size: cover;\n",
        "    }\n",
        "    </style>\n",
        "    ''' % bin_str\n",
        "    st.markdown(page_bg_img, unsafe_allow_html=True)\n",
        "    return\n",
        "\n",
        "\n",
        "set_background('images/background-dark2.jpeg')\n",
        "\n",
        "\n",
        "######################################\n",
        "\n",
        "######################################\n",
        "# Sidebar Options\n",
        "def sidebar_bg(side_bg):\n",
        "    side_bg_ext = 'png'\n",
        "    st.markdown(\n",
        "        f\"\"\"\n",
        "      <style>\n",
        "      [data-testid=\"stSidebar\"] > div:first-child {{\n",
        "          background: url(data:image/{side_bg_ext};base64,{base64.b64encode(open(side_bg, \"rb\").read()).decode()});\n",
        "      }}\n",
        "      </style>\n",
        "      \"\"\",\n",
        "        unsafe_allow_html=True,\n",
        "    )\n",
        "\n",
        "\n",
        "side_bg = './images/sidebar2-dark.png'\n",
        "sidebar_bg(side_bg)\n",
        "\n",
        "# sidebar logo\n",
        "st.markdown(\n",
        "    \"\"\"\n",
        "    <style>\n",
        "        [data-testid=stSidebar] [data-testid=stImage]{\n",
        "            text-align: center;\n",
        "            display: block;\n",
        "            margin-left: auto;\n",
        "            margin-right: auto;\n",
        "            width: 100%;\n",
        "        }\n",
        "    </style>\n",
        "    \"\"\", unsafe_allow_html=True\n",
        ")\n",
        "\n",
        "with st.sidebar:\n",
        "    st.image(\"images/elastic_logo_transp_100.png\")\n",
        "\n",
        "######################################\n",
        "# expander markdown\n",
        "st.markdown(\n",
        "    '''\n",
        "    <style>\n",
        "    .streamlit-expanderHeader {\n",
        "        background-color: gray;\n",
        "        color: black; # Adjust this for expander header color\n",
        "    }\n",
        "    .streamlit-expanderContent {\n",
        "        background-color: white;\n",
        "        color: black; # Expander content color\n",
        "    }\n",
        "    </style>\n",
        "    ''',\n",
        "    unsafe_allow_html=True\n",
        ")\n",
        "\n",
        "######################################\n",
        "\n",
        "@st.cache_resource\n",
        "def initOpenAI():\n",
        "    #if using the Elastic AI proxy, then generate the correct API key\n",
        "    if os.environ['ELASTIC_PROXY'] == \"True\":\n",
        "        #generate and share \"your\" unique hash\n",
        "        os.environ['USER_HASH'] = secrets.token_hex(nbytes=6)\n",
        "        print(f\"Your unique user hash is: {os.environ['USER_HASH']}\")\n",
        "        #get the current API key and combine with your hash\n",
        "        os.environ['OPENAI_API_KEY'] = f\"{os.environ['OPENAI_API_KEY']} {os.environ['USER_HASH']}\"\n",
        "    else:\n",
        "        openai.api_type = os.environ['OPENAI_API_TYPE']\n",
        "        openai.api_version = os.environ['OPENAI_API_VERSION']\n",
        "\n",
        "    # Configure OpenAI client\n",
        "    openai.api_key = os.environ['OPENAI_API_KEY']\n",
        "    openai.api_base = os.environ['OPENAI_API_BASE']\n",
        "    openai.default_model = os.environ['OPENAI_API_ENGINE']\n",
        "    openai.verify_ssl_certs = False\n",
        "    client = OpenAI(api_key=openai.api_key, base_url=openai.api_base)\n",
        "    return client\n",
        "\n",
        "openAIClient = initOpenAI()\n",
        "\n",
        "# Initialize Elasticsearch and APM clients\n",
        "# Configure APM and Elasticsearch clients\n",
        "@st.cache_resource\n",
        "def initElastic():\n",
        "    os.environ['ELASTIC_APM_SERVICE_NAME'] = \"genai_workshop_v2_lab_2-2\"\n",
        "    apmclient = elasticapm.Client()\n",
        "    elasticapm.instrument()\n",
        "\n",
        "    if 'ELASTIC_CLOUD_ID_W' in os.environ:\n",
        "        es = Elasticsearch(\n",
        "            cloud_id=os.environ['ELASTIC_CLOUD_ID_W'],\n",
        "            api_key=(os.environ['ELASTIC_APIKEY_ID_W']),\n",
        "            request_timeout=30\n",
        "        )\n",
        "    else:\n",
        "        es = Elasticsearch(\n",
        "            os.environ['ELASTIC_URL'],\n",
        "            basic_auth=(os.environ['ELASTIC_USER'], os.environ['ELASTIC_PASSWORD']),\n",
        "            request_timeout=30\n",
        "        )\n",
        "\n",
        "    return apmclient, es\n",
        "\n",
        "\n",
        "apmclient, es = initElastic()\n",
        "\n",
        "# Set our data index\n",
        "index = os.environ['ELASTIC_INDEX_DOCS_W']\n",
        "\n",
        "###############################################################\n",
        "# Similarity Cache functions\n",
        "# move to env if time\n",
        "cache_index = \"wikipedia-cache\"\n",
        "\n",
        "\n",
        "def clear_es_cache(es):\n",
        "    print('clearing cache')\n",
        "    match_all_query = {\"query\": {\"match_all\": {}}}\n",
        "    clear_response = es.delete_by_query(index=cache_index, body=match_all_query)\n",
        "    return clear_response\n",
        "\n",
        "\n",
        "@elasticapm.capture_span(\"cache_search\")\n",
        "def cache_query(cache, prompt_text, similarity_threshold=0.5):\n",
        "    hit = cache.query(prompt_text=prompt_text, similarity_threshold=similarity_threshold)\n",
        "\n",
        "    if hit:\n",
        "        st.sidebar.markdown('`Cache Match Found`')\n",
        "    else:\n",
        "        st.sidebar.markdown('`Cache Miss`')\n",
        "\n",
        "    return hit\n",
        "\n",
        "\n",
        "@elasticapm.capture_span(\"add_to_cache\")\n",
        "def add_to_cache(cache, prompt, response):\n",
        "    st.sidebar.markdown('`Adding response to cache`')\n",
        "    print('adding to cache')\n",
        "    print(prompt)\n",
        "    print(response)\n",
        "    resp = cache.add(prompt=prompt, response=response)\n",
        "    st.markdown(resp)\n",
        "    return resp\n",
        "\n",
        "\n",
        "def init_elastic_cache():\n",
        "    # Init Elasticsearch Cache\n",
        "    # Only want to attempt to create the index on first run\n",
        "    cache = ElasticsearchLLMCache(es_client=es,\n",
        "                                  index_name=cache_index,\n",
        "                                  create_index=False  # setting only because of Streamlit behavior\n",
        "                                  )\n",
        "    st.sidebar.markdown('`creating Elasticsearch Cache`')\n",
        "\n",
        "    if \"index_created\" not in st.session_state:\n",
        "\n",
        "        st.sidebar.markdown('`running create_index`')\n",
        "        cache.create_index(768)\n",
        "\n",
        "        # Set the flag so it doesn't run every time\n",
        "        st.session_state.index_created = True\n",
        "    else:\n",
        "        st.sidebar.markdown('`index already created, skipping`')\n",
        "\n",
        "    return cache\n",
        "\n",
        "\n",
        "def calc_similarity(score, func_type='dot_product'):\n",
        "    if func_type == 'dot_product':\n",
        "        return (score + 1) / 2\n",
        "    elif func_type == 'cosine':\n",
        "        return (1 + score) / 2\n",
        "    elif func_type == 'l2_norm':\n",
        "        return 1 / (1 + score ^ 2)\n",
        "    else:\n",
        "        return score\n",
        "\n",
        "\n",
        "# l2_norm: sqrt((1 / _score) - 1)\n",
        "# cosine: (2 * _score) - 1\n",
        "# dot_product: (2 * _score) - 1\n",
        "# max_inner_product:\n",
        "# _score < 1: 1 - (1 / _score)\n",
        "# _score >= 1: _score - 1\n",
        "\n",
        "\n",
        "###############################################################\n",
        "\n",
        "\n",
        "def get_bm25_query(query_text, augment_method):\n",
        "    if augment_method == \"Full Text\":\n",
        "        return {\n",
        "            \"match\": {\n",
        "                \"text\": query_text\n",
        "            }\n",
        "        }\n",
        "    elif augment_method == \"Matching Chunk\":\n",
        "        return {\n",
        "            \"nested\": {\n",
        "                \"path\": \"passages\",\n",
        "                \"query\": {\n",
        "                    \"bool\": {\n",
        "                        \"must\": [\n",
        "                            {\n",
        "                                \"match\": {\n",
        "                                    \"passages.text\": query_text\n",
        "                                }\n",
        "                            }\n",
        "                        ]\n",
        "                    }\n",
        "                },\n",
        "                \"inner_hits\": {\n",
        "                    \"_source\": False,\n",
        "                    \"fields\": [\n",
        "                        \"passages.text\"\n",
        "                    ]\n",
        "                }\n",
        "\n",
        "            }\n",
        "        }\n",
        "\n",
        "\n",
        "# Run an Elasticsearch query using BM25 relevance scoring\n",
        "@elasticapm.capture_span(\"bm25_search\")\n",
        "def search_bm25(query_text,\n",
        "                es,\n",
        "                size=1,\n",
        "                augment_method=\"Full Text\",\n",
        "                use_hybrid=False  # always false - use semantic opt for hybrid\n",
        "                ):\n",
        "    fields = [\n",
        "        \"text\",\n",
        "        \"title\",\n",
        "    ]\n",
        "\n",
        "    resp = es.search(index=index,\n",
        "                     query=get_bm25_query(query_text, augment_method),\n",
        "                     fields=fields,\n",
        "                     size=size,\n",
        "                     source=False)\n",
        "    # print(resp)\n",
        "    body = resp\n",
        "    url = 'nothing'\n",
        "\n",
        "    return body, url\n",
        "\n",
        "\n",
        "@elasticapm.capture_span(\"knn_search\")\n",
        "def search_knn(query_text,\n",
        "               es,\n",
        "               size=1,\n",
        "               augment_method=\"Full Text\",\n",
        "               use_hybrid=False\n",
        "               ):\n",
        "    fields = [\n",
        "        \"title\",\n",
        "        \"text\"\n",
        "    ]\n",
        "\n",
        "    knn = {\n",
        "        \"inner_hits\": {\n",
        "            \"_source\": False,\n",
        "            \"fields\": [\n",
        "                \"passages.text\"\n",
        "            ]\n",
        "        },\n",
        "        \"field\": \"passages.embeddings\",\n",
        "        \"k\": size,\n",
        "        \"num_candidates\": 100,\n",
        "        \"query_vector_builder\": {\n",
        "            \"text_embedding\": {\n",
        "                \"model_id\": \"sentence-transformers__all-distilroberta-v1\",\n",
        "                \"model_text\": query_text\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    rank = {\"rrf\": {}} if use_hybrid else None\n",
        "\n",
        "    # need to get the bm25 query if we are using hybrid\n",
        "    if use_hybrid:\n",
        "        print('using hybrid with augment method %s' % augment_method)\n",
        "        query = get_bm25_query(query_text, augment_method)\n",
        "        print(query)\n",
        "        if augment_method == \"Matching Chunk\":\n",
        "            del query['nested']['inner_hits']\n",
        "    else:\n",
        "        print('not using hybrid')\n",
        "        query = None\n",
        "\n",
        "    print(query)\n",
        "    print(knn)\n",
        "\n",
        "    resp = es.search(index=index,\n",
        "                     knn=knn,\n",
        "                     query=query,\n",
        "                     fields=fields,\n",
        "                     size=size,\n",
        "                     rank=rank,\n",
        "                     source=False)\n",
        "\n",
        "    return resp, None\n",
        "\n",
        "\n",
        "def truncate_text(text, max_tokens):\n",
        "    tokens = text.split()\n",
        "    if len(tokens) <= max_tokens:\n",
        "        return text\n",
        "\n",
        "    return ' '.join(tokens[:max_tokens])\n",
        "\n",
        "\n",
        "def build_text_obj(resp, aug_method):\n",
        "\n",
        "    tobj = {}\n",
        "\n",
        "    for hit in resp['hits']['hits']:\n",
        "        # tobj[hit['fields']['title'][0]] = []\n",
        "        title = hit['fields']['title'][0]\n",
        "        tobj.setdefault(title, [])\n",
        "\n",
        "        if aug_method == \"Matching Chunk\":\n",
        "            print('hit')\n",
        "            print(hit)\n",
        "            # tobj['passages'] = []\n",
        "            for ihit in hit['inner_hits']['passages']['hits']['hits']:\n",
        "                tobj[title].append(\n",
        "                    {'passage': ihit['fields']['passages'][0]['text'][0],\n",
        "                     '_score': ihit['_score']}\n",
        "                )\n",
        "        elif aug_method == \"Full Text\":\n",
        "            tobj[title].append(\n",
        "                hit['fields']\n",
        "            )\n",
        "\n",
        "    return tobj\n",
        "\n",
        "\n",
        "def generate_response(query,\n",
        "                      es,\n",
        "                      search_method,\n",
        "                      custom_prompt,\n",
        "                      negative_response,\n",
        "                      show_prompt, size=1,\n",
        "                      augment_method=\"Full Text\",\n",
        "                      use_hybrid=False,\n",
        "                      show_es_response=True,\n",
        "                      show_es_augment=True,\n",
        "                      ):\n",
        "\n",
        "    # Perform the search based on the specified method\n",
        "    search_functions = {\n",
        "        'bm25': {'method': search_bm25, 'display': 'Lexical Search'},\n",
        "        'knn': {'method': search_knn, 'display': 'Semantic Search'}\n",
        "    }\n",
        "    search_func = search_functions.get(search_method)['method']\n",
        "    if not search_func:\n",
        "        raise ValueError(f\"Invalid search method: {search_method}\")\n",
        "\n",
        "    # Perform the search and format the docs\n",
        "    response, url = search_func(query, es, size, augment_method, use_hybrid)\n",
        "    es_time = time.time()\n",
        "    augment_text = build_text_obj(response, augment_method)\n",
        "\n",
        "    res_col1, res_col2 = st.columns(2)\n",
        "    # Display the search results from ES\n",
        "    with res_col2:\n",
        "        st.header(':rainbow[Elasticsearch Response]')\n",
        "        st.subheader(':orange[Search Settings]')\n",
        "        st.write(':gray[Search Method:] :blue[%s]' % search_functions.get(search_method)['display'])\n",
        "        st.write(':gray[Size Setting:] :blue[%s]' % size)\n",
        "        st.write(':gray[Augment Setting:] :blue[%s]' % augment_method)\n",
        "        st.write(':gray[Using Hybrid:] :blue[%s]' % (\n",
        "            'Not Applicable with Lexical' if search_method == 'bm25' else use_hybrid))\n",
        "\n",
        "        st.subheader(':green[Augment Chunk(s) from Elasticsearch]')\n",
        "        if show_es_augment:\n",
        "            st.json(dict(augment_text))\n",
        "        else:\n",
        "            st.write(':blue[Show Augment Disabled]')\n",
        "\n",
        "        st.subheader(':violet[Elasticsearch Response]')\n",
        "        if show_es_response:\n",
        "            st.json(dict(response))\n",
        "        else:\n",
        "            st.write(':blue[Response Received]')\n",
        "\n",
        "    formatted_prompt = custom_prompt.replace(\"$query\", query).replace(\"$response\", str(augment_text)).replace(\n",
        "        \"$negResponse\", negative_response)\n",
        "\n",
        "    with res_col1:\n",
        "        st.header(':orange[GenAI Response]')\n",
        "\n",
        "        chat_response = chat_gpt(formatted_prompt, system_prompt=\"You are a helpful assistant.\")\n",
        "\n",
        "        # Display assistant response in chat message container\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            message_placeholder = st.empty()\n",
        "            full_response = \"\"\n",
        "            for chunk in chat_response.split():\n",
        "                full_response += chunk + \" \"\n",
        "                time.sleep(0.02)\n",
        "                # Add a blinking cursor to simulate typing\n",
        "                message_placeholder.markdown(full_response + \"▌\")\n",
        "            message_placeholder.markdown(full_response)\n",
        "\n",
        "    # Display results\n",
        "    if show_prompt:\n",
        "        st.text(\"Full prompt sent to ChatGPT:\")\n",
        "        st.text(wrap_text(formatted_prompt, 70))\n",
        "\n",
        "    if negative_response not in chat_response:\n",
        "        pass\n",
        "    else:\n",
        "        chat_response = None\n",
        "\n",
        "    return es_time, chat_response\n",
        "\n",
        "def count_tokens(messages, model=\"gpt-35-turbo\"):\n",
        "    if \"gpt-3.5-turbo\" in model or \"gpt-35-turbo\" in model:\n",
        "        model = \"gpt-3.5-turbo-0613\"\n",
        "    elif \"gpt-4\" in model:\n",
        "        model=\"gpt-4-0613\"\n",
        "\n",
        "    try:\n",
        "        encoding = tiktoken.encoding_for_model(model)\n",
        "    except KeyError:\n",
        "        print(\"Warning: model not found. Using gpt-3.5-turbo-0613 encoding.\")\n",
        "        encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo-0613\")\n",
        "\n",
        "    if isinstance(messages, str):\n",
        "        return len(encoding.encode(messages))\n",
        "    else:\n",
        "        tokens_per_message = 3\n",
        "        tokens_per_name = 1\n",
        "\n",
        "        num_tokens = 0\n",
        "        for message in messages:\n",
        "            num_tokens += tokens_per_message\n",
        "            for key, value in message.items():\n",
        "                num_tokens += len(encoding.encode(value))\n",
        "                if key == \"name\":\n",
        "                    num_tokens += tokens_per_name\n",
        "        num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
        "        return num_tokens\n",
        "\n",
        "def chat_gpt(user_prompt, system_prompt):\n",
        "    \"\"\"\n",
        "    Generates a response from ChatGPT based on the given user and system prompts.\n",
        "    \"\"\"\n",
        "    max_tokens = 1024\n",
        "    max_context_tokens = 4000\n",
        "    safety_margin = 5\n",
        "\n",
        "    # Truncate the prompt content to fit within the model's context length\n",
        "    truncated_prompt = truncate_text(user_prompt, max_context_tokens - max_tokens - safety_margin)\n",
        "\n",
        "    # Prepare the messages for the ChatGPT API\n",
        "    messages = [{\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": truncated_prompt}]\n",
        "\n",
        "    full_response = \"\"\n",
        "    for response in openAIClient.chat.completions.create(\n",
        "        model=openai.default_model,\n",
        "        temperature=0,\n",
        "        messages=messages,\n",
        "        stream=True\n",
        "    ):\n",
        "        full_response += (response.choices[0].delta.content or \"\")\n",
        "\n",
        "    # APM: add metadata labels of data we want to capture\n",
        "    elasticapm.label(model = openai.default_model)\n",
        "    elasticapm.label(prompt = user_prompt)\n",
        "    elasticapm.label(prompt_tokens = count_tokens(messages, model=openai.default_model))\n",
        "    elasticapm.label(response_tokens = count_tokens(full_response, model=openai.default_model))\n",
        "    elasticapm.label(total_tokens = count_tokens(messages, model=openai.default_model) + count_tokens(full_response, model=openai.default_model))\n",
        "    if 'USER_HASH' in os.environ: elasticapm.label(user = os.environ['USER_HASH'])\n",
        "\n",
        "    return full_response\n",
        "\n",
        "\n",
        "# Main chat form\n",
        "st.title(\"Wikipedia RAG Demo Platform\")\n",
        "\n",
        "# Define the default prompt and negative response\n",
        "default_prompt_intro = \"Answer this question:\"\n",
        "default_response_instructions = (\"using only the information from the wikipedia documents included and nothing \"\n",
        "                                 \"else.\\nwikipedia_docs: $response\\n\")\n",
        "default_negative_response = (\"If the answer is not provided in the included documentation. You are to ONLY reply with \"\n",
        "                             \"'I'm unable to answer the question based on the information I have from wikipedia' and \"\n",
        "                             \"nothing else.\")\n",
        "\n",
        "with st.form(\"chat_form\"):\n",
        "    query = st.text_input(\"Ask a question in Wikipedia:\",\n",
        "                          placeholder='Sample Question: Who is Batman?')\n",
        "\n",
        "    opt_col1, opt_col2 = st.columns(2)\n",
        "    with opt_col1:\n",
        "        with st.expander(\"Customize Prompt Template\"):\n",
        "            prompt_intro = st.text_area(\"Introduction/context of the prompt:\", value=default_prompt_intro)\n",
        "            prompt_query_placeholder = st.text_area(\"Placeholder for the user's query:\", value=\"$query\")\n",
        "            prompt_response_placeholder = st.text_area(\"Placeholder for the Elasticsearch response:\",\n",
        "                                                       value=default_response_instructions)\n",
        "            prompt_negative_response = st.text_area(\"Negative response placeholder:\", value=default_negative_response)\n",
        "            prompt_closing = st.text_area(\"Closing remarks of the prompt:\",\n",
        "                                          value=\"Format the answer in complete markdown code format.\")\n",
        "\n",
        "            combined_prompt = f\"{prompt_intro}\\n{prompt_query_placeholder}\\n{prompt_response_placeholder}\\n{prompt_negative_response}\\n{prompt_closing}\"\n",
        "            st.text_area(\"Preview of your custom prompt:\", value=combined_prompt, disabled=True)\n",
        "\n",
        "    with opt_col2:\n",
        "        with st.expander(\"Retrieval Search and Display Options\"):\n",
        "            st.subheader(\"Retrieval Options\")\n",
        "            ret_1, ret_2 = st.columns(2)\n",
        "            with ret_1:\n",
        "                search_method = st.radio(\"Search Method\", (\"Semantic Search\", \"Lexical Search\"))\n",
        "                augment_method = st.radio(\"Augment Method\", (\"Full Text\", \"Matching Chunk\"))\n",
        "            with ret_2:\n",
        "                # TODO this should update the title based on the augment_method\n",
        "                doc_count_title = \"Number of docs or chunks to Augment with\" if augment_method == \"Full Text\" else \"Number of Matching Chunks to Retrieve\"\n",
        "                doc_count = st.slider(doc_count_title, min_value=1, max_value=5, value=1)\n",
        "\n",
        "                use_hybrid = st.checkbox('Use Hybrid Search')\n",
        "\n",
        "            st.divider()\n",
        "\n",
        "            st.subheader(\"Display Options\")\n",
        "            show_es_augment = st.checkbox('Show Elasticsearch Augment Text', value=True)\n",
        "            show_es_response = st.checkbox('Show Elasticsearch Response', value=True)\n",
        "            show_full_prompt = st.checkbox('Show Full Prompt Sent to LLM')\n",
        "\n",
        "            st.divider()\n",
        "\n",
        "            st.subheader(\"Caching Options\")\n",
        "            cache_1, cache_2 = st.columns(2)\n",
        "            with cache_1:\n",
        "                use_cache = st.checkbox('Use Similarity Cache')\n",
        "                # Slider for adjusting similarity threshold\n",
        "                similarity_threshold_selection = st.slider(\n",
        "                    \"Select Similarity Threshold (dot_product - Higher Similarity means closer)\",\n",
        "                    min_value=0.0, max_value=2.0,\n",
        "                    value=0.5, step=0.01)\n",
        "\n",
        "            with cache_2:\n",
        "                clear_cache_butt = st.form_submit_button(':red[Clear Similarity Cache]')\n",
        "\n",
        "    col1, col2 = st.columns(2)\n",
        "    with col1:\n",
        "        answer_button = st.form_submit_button(\"Find my answer!\")\n",
        "\n",
        "# Clear Cache Button\n",
        "if clear_cache_butt:\n",
        "    st.session_state.clear_cache_clicked = True\n",
        "\n",
        "# Confirmation step\n",
        "if st.session_state.get(\"clear_cache_clicked\", False):\n",
        "    apmclient.begin_transaction(\"clear_cache\")\n",
        "    elasticapm.label(action=\"clear_cache\")\n",
        "\n",
        "    # Start timing\n",
        "    start_time = time.time()\n",
        "\n",
        "    if st.button(\":red[Confirm Clear Cache]\"):\n",
        "        print('clear cache clicked')\n",
        "        # TODO if index doesn't exist, catch exception then create it\n",
        "        response = clear_es_cache(es)\n",
        "        st.success(\"Cache cleared successfully!\", icon=\"🤯\")\n",
        "        st.session_state.clear_cache_clicked = False  # Reset the state\n",
        "\n",
        "    apmclient.end_transaction(\"clear_cache\", \"success\")\n",
        "\n",
        "if answer_button:\n",
        "    search_method = \"knn\" if search_method == \"Semantic Search\" else \"bm25\"\n",
        "\n",
        "    apmclient.begin_transaction(\"query\")\n",
        "    elasticapm.label(search_method=search_method)\n",
        "    elasticapm.label(query=query)\n",
        "\n",
        "    # Start timing\n",
        "    start_time = time.time()\n",
        "\n",
        "    if query == \"\":\n",
        "        st.error(\"Please enter a question in the Question Box.\")\n",
        "        apmclient.end_transaction(\"query\", \"failure\")\n",
        "\n",
        "    else:\n",
        "        if use_cache:\n",
        "            cache = init_elastic_cache()\n",
        "\n",
        "            # check the llm cache first\n",
        "            st.sidebar.markdown('`Checking ES Cache`')\n",
        "            cache_check = cache_query(cache,\n",
        "                                      prompt_text=query,\n",
        "                                      similarity_threshold=similarity_threshold_selection\n",
        "                                      )\n",
        "            # st.markdown(cache_check)\n",
        "        else:\n",
        "            cache_check = None\n",
        "            st.sidebar.markdown('`Skipping ES Cache`')\n",
        "\n",
        "        try:\n",
        "\n",
        "            if cache_check:\n",
        "                es_time = time.time()\n",
        "                st.sidebar.markdown('`cache match, using cached results`')\n",
        "                st.subheader('Response from Cache')\n",
        "                s_score = calc_similarity(cache_check['_score'], func_type='dot_product')\n",
        "                st.code(f\"Similarity Value: {s_score:.5f}\")\n",
        "\n",
        "                # Display response from LLM\n",
        "                st.header('LLM Response')\n",
        "                # st.markdown(cache_check['response'][0])\n",
        "                with st.chat_message(\"assistant\"):\n",
        "                    message_placeholder = st.empty()\n",
        "                    full_response = \"\"\n",
        "                    for chunk in cache_check['response'][0].split():\n",
        "                        full_response += chunk + \" \"\n",
        "                        time.sleep(0.02)\n",
        "                        # Add a blinking cursor to simulate typing\n",
        "                        message_placeholder.markdown(full_response + \"▌\")\n",
        "                    message_placeholder.markdown(full_response)\n",
        "\n",
        "                llmAnswer = None  # no need to recache the answer\n",
        "\n",
        "            else:\n",
        "                # Use combined_prompt and show_full_prompt as arguments\n",
        "                es_time, llmAnswer = generate_response(query,\n",
        "                                                       es,\n",
        "                                                       search_method,\n",
        "                                                       combined_prompt,\n",
        "                                                       prompt_negative_response,\n",
        "                                                       show_full_prompt,\n",
        "                                                       doc_count,\n",
        "                                                       augment_method,\n",
        "                                                       use_hybrid,\n",
        "                                                       show_es_response,\n",
        "                                                       show_es_augment,\n",
        "                                                       )\n",
        "            apmclient.end_transaction(\"query\", \"success\")\n",
        "\n",
        "            if use_cache and llmAnswer:\n",
        "                if \"I'm unable to answer the question\" in llmAnswer:\n",
        "                    st.sidebar.markdown('`unable to answer, not adding to cache`')\n",
        "                else:\n",
        "                    st.sidebar.markdown('`adding prompt and response to cache`')\n",
        "                    add_to_cache(cache, query, llmAnswer)\n",
        "\n",
        "            # End timing and print the elapsed time\n",
        "            elapsed_time = time.time() - start_time\n",
        "            es_elapsed_time = es_time - start_time\n",
        "\n",
        "            ct1, ct2 = st.columns(2)\n",
        "            with ct1:\n",
        "                st.subheader(\"GenAI Time taken: :red[%.2f seconds]\" % elapsed_time)\n",
        "\n",
        "            with ct2:\n",
        "                st.subheader(\"ES Query Time taken: :green[%.2f seconds]\" % es_elapsed_time)\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"An error occurred: {str(e)}\")\n",
        "            apmclient.end_transaction(\"query\", \"failure\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wu0KfS0ESf6e"
      },
      "source": [
        "#### Run the RAG Application\n",
        "Running this cell will start local tunnel and generate a random URL\n",
        "\n",
        "1. Run this cell\n",
        "2. Copy the IP address on the first line\n",
        "3. Open the generated URL\n",
        "4. Paste the copied IP into the input box *Endpoint IP*\n",
        "\n",
        "This will then start the Rag Application"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLzXT8oajcvo"
      },
      "outputs": [],
      "source": [
        "! git clone \"https://github.com/elastic/genai-workshop-colab.git\"\n",
        "! cd genai-workshop-colab;  git checkout main; cd ..; cp -r ./genai-workshop-colab/notebooks/images images; cp -r ./genai-workshop-colab/notebooks/.streamlit .streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHIHFID3NBXa"
      },
      "outputs": [],
      "source": [
        "!streamlit run app.py &>/content/logs.txt & npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
