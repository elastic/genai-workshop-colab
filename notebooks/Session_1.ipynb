{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeTXscrkNOF1"
      },
      "source": [
        "# [Session 1] Introduction to Generative AI\n",
        "\n",
        "This notebook includes *all* the labs of Session 1.  They are meant to be run in order. If you finish early, try playing around with the LLMs loaded to that point.\n",
        "\n",
        "## Tips on using Jupyter Notebooks and Google Colab\n",
        "\n",
        "* Notebooks are just .ipynb files, you can run them locally in any python dev environment if you'd like. We are running in Google Colab to keep things simple (and well tested) for this course.\n",
        "\n",
        "* In Google Colab, each notebook has its own independent execution environment.\n",
        "  * You can see the current in memory and session variables on the left in the ```{X}``` menu\n",
        "  * You can see the file system in menu with the folder icon\n",
        "  * You can connect to a runtime and monitor RAM and Disk using menu in the top right.\n",
        "  * Limited GPU enabled instances are availble, but not all notebooks will need them.\n",
        "\n",
        "* Notebooks have Markdown and Code snippets\n",
        "  * You can access the shell of the coding environment with a ```!``` command\n",
        "  * Run each code sample in order. Notebooks will usually import libraries they need as the workshop progresses\n",
        "\n",
        "* You can always restart your code environment and rerun if you get into trouble\n",
        "  * in-memory variables are lost on a **restart**\n",
        "  * in-memory variables and the file system are lost on a **disconnect**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup Environment\n",
        "The following code loads the environment variables required to run this notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "FILE=\"Session_1\"\n",
        "\n",
        "! pip install -qqq git+https://github.com/elastic/notebook-workshop-loader.git@main\n",
        "from notebookworkshoploader import loader\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "if os.path.isfile(\"../env\"):\n",
        "    load_dotenv(\"../env\", override=True)\n",
        "    print('Successfully loaded environment variables from local env file')\n",
        "else:\n",
        "    loader.load_remote_env(file=FILE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sm8uPLZxNOF3"
      },
      "source": [
        "## Lab 1-1: Introduction and Transformer Models\n",
        "\n",
        "In this lab we will\n",
        "* Intro to Google Colab - Hello World, importing python libraries\n",
        "* Caching the download of a smaller LLM\n",
        "* Using a basic transformer models locally\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Learning Colab"
      ],
      "metadata": {
        "id": "j9xHoMleO_xz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 1: Hit play on the next code **sample**"
      ],
      "metadata": {
        "id": "HqlXsCGMN6TC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0WRcBgDNOF3"
      },
      "outputs": [],
      "source": [
        "print(\"Hello World\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P85HsyKmNOF4"
      },
      "source": [
        "#### Step 2: Use ! to execute a shell command"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfW6H9meNOF4"
      },
      "outputs": [],
      "source": [
        "! echo \"The shell thinks the Current Directory is: $(pwd)\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting some python dependencies"
      ],
      "metadata": {
        "id": "mZ_VBiFPPIM5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSXuTJelNOF5"
      },
      "source": [
        "#### Step 3: Environment setup\n",
        "\n",
        "First let us import some Python libraries we'll use in the first lab module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FivGgCcXNOF5"
      },
      "outputs": [],
      "source": [
        "! pip install --upgrade pip\n",
        "! pip install -q --no-cache-dir torch\n",
        "! pip install -q --upgrade transformers\n",
        "! pip install -q xformers\n",
        "! pip install -q python-dotenv\n",
        "! pip install -q \"openai<1.0.0\"           ## for later in the lab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kk2GXx6ANOF5"
      },
      "source": [
        "#### Step 4: Utility functions\n",
        "Some utility functions that are good to keep on hand"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kVJFIJxNOF5"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "# pretty printing JSON objects\n",
        "def json_pretty(input_object):\n",
        "  print(json.dumps(input_object, indent=4))\n",
        "\n",
        "\n",
        "import textwrap\n",
        "# wrap text when printing, because colab scrolls output to the right too much\n",
        "def wrap_text(text, width):\n",
        "    wrapped_text = textwrap.wrap(text, width)\n",
        "    return '\\n'.join(wrapped_text)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading, Caching, and Prepping a Model"
      ],
      "metadata": {
        "id": "IZalQ_D_PXCg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIPsPhYuNOF5"
      },
      "source": [
        "#### Step 5: Download sentiment analysis model\n",
        "\n",
        "We'll use the Huggingface Transformer library to download and ready an Open Source model called DistilBERT which can be used for sentiment analysis.\n",
        "\n",
        "* Details of the model can be found on its [Hugging Face Page](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n",
        "* This model is pretrained to determine if an input text is of *POSITIVE* or *NEGATIVE* sentiment and makes a good intro example to AI models.\n",
        "* Note we are caching the model files in a folder called ```llm_download_cache``` which will help us not have to re-download the files again within the connection to this runtime. You can see the download in the filesystem (using the left hand side menu)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rg4o4viHNOF5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import tqdm\n",
        "import json\n",
        "import os\n",
        "from transformers import (pipeline,\n",
        "  DistilBertTokenizer,\n",
        "  DistilBertForSequenceClassification)\n",
        "\n",
        "# Set the cache directory\n",
        "cache_directory = \"llm_download_cache\"\n",
        "\n",
        "# Create the cache directory if it doesn't exist\n",
        "if not os.path.exists(cache_directory):\n",
        "    os.makedirs(cache_directory)\n",
        "\n",
        "model_id = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "sentiment_tokenizer = DistilBertTokenizer.from_pretrained(\n",
        "    model_id, cache_dir=cache_directory)\n",
        "sentiment_model = DistilBertForSequenceClassification.from_pretrained(\n",
        "    model_id, cache_dir=cache_directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1N1DeQ4SNOF5"
      },
      "source": [
        "Okay! let's run the model ```sentiment_model``` on two pieces of sample text.\n",
        "\n",
        "#### Step 6: Run sentiment analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfJ4WqGLNOF6"
      },
      "outputs": [],
      "source": [
        "## With the distilbert model downloaded and cached we can call it for\n",
        "## sentiment analysis\n",
        "\n",
        "# Define the sentiment analysis pipeline\n",
        "sentiment_classifier = pipeline(\"sentiment-analysis\",\n",
        "                                model=sentiment_model,\n",
        "                                tokenizer=sentiment_tokenizer,\n",
        "                                device='cpu')\n",
        "#two samples\n",
        "classifier_results = sentiment_classifier([\n",
        "    \"My dog is so cute, I love him.\",\n",
        "\n",
        "    \"I am very sorry to inform you that the tax\\\n",
        "     administration has decided to audit you.\"\n",
        "])\n",
        "\n",
        "json_pretty(classifier_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLn39tLNNOF6"
      },
      "source": [
        "### ðŸ«µ Try it yourself - Get Creative ðŸ«µ\n",
        "Try some of your own examples.\n",
        "Note, AI models are subject to bias. The model card for this model goes into pretty good detail on the issue. [Read more here](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english#risks-limitations-and-biases)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYS-5TMPNOF6"
      },
      "outputs": [],
      "source": [
        "your_classifier_results = sentiment_classifier([\n",
        "    \"CHANGE ME\",\n",
        "    \"CHANGE ME\"\n",
        "])\n",
        "json_pretty(your_classifier_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generative LLM - Simple and Local"
      ],
      "metadata": {
        "id": "1F5ytSTdPoCo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5f1vIeeNOF6"
      },
      "source": [
        "#### Step 7: Download Flan T5\n",
        "\n",
        "Let's start with the Hello World of generative AI examples: completing a sentence. For this we'll install Google's Flan-T5 model.\n",
        "\n",
        "Note, while this is a smaller checkpoint of the model, it is still a 3GB download.  We'll cache the files in the same folder.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6kqG5siNOF6"
      },
      "outputs": [],
      "source": [
        "## Let's play with something a little bigger that can do a text completion\n",
        "## This is a 3 GB download and takes some RAM to run, but it works CPU only\n",
        "\n",
        "from transformers import AutoTokenizer, pipeline, AutoModelForSeq2SeqLM\n",
        "\n",
        "model_id = 'google/flan-t5-large'\n",
        "llm_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_id, cache_dir=cache_directory)\n",
        "llm_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    model_id, cache_dir=cache_directory)\n",
        "\n",
        "llm_pipe = pipeline(\n",
        "        \"text2text-generation\",\n",
        "        model=llm_model,\n",
        "        tokenizer=llm_tokenizer,\n",
        "        max_length=100\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSCILNELNOF6"
      },
      "source": [
        "#### Step 8: Generate text completions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gxpdh9UtNOF7"
      },
      "outputs": [],
      "source": [
        "countries = [\"United Kingdom\",\n",
        "             \"France\",\n",
        "             \"People's Republic of China\",\n",
        "             \"United States\",\n",
        "             \"Ecuador\",\n",
        "             \"Faketopia\"]\n",
        "\n",
        "for country in countries:\n",
        "    input_text = f\"The capital of the {country} is\"\n",
        "    output = llm_pipe(input_text)\n",
        "    completed_sentence = f\"\\033[94m{input_text}\\033[0m {output[0]['generated_text']}\"\n",
        "    print(completed_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpsbegewNOF7"
      },
      "source": [
        "### ðŸ«µ Try it yourself - Get Creative ðŸ«µ\n",
        "Try some of your own examples.\n",
        "This thing isn't super smart without fine tuning, but it can handle some light context injection and prompt engineering. We'll learn more about those subjects in later modules.\n",
        "\n",
        "Notice the difference between asking a specific question and phrasing a completion\n",
        "* \"Who is the Prime Minister of the UK?\"\n",
        "* \"The current Prime Minister of the united kingdom is \""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6GhGplrNOF7"
      },
      "outputs": [],
      "source": [
        "prompt_text = \"The current Prime Minister of the united kingdom is \"\n",
        "output = llm_pipe(prompt_text)\n",
        "completed_prompt = f\"\\033[94m{prompt_text}\\033[0m {output[0]['generated_text']}\"\n",
        "print(completed_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bvjzCiANOF7"
      },
      "source": [
        "ðŸ›‘ Stop Here ðŸ›‘\n",
        "\n",
        "This Ends Lab 1-1\n",
        "<hr/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_y2q5sxMNOF7"
      },
      "source": [
        "## Lab 1-2: Prompts and Context Windows\n",
        "\n",
        "* Using langchain with local LLM\n",
        "* Connect to Open AI\n",
        "* Using a memory window to create a txt-only GPT conversation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic chat completion"
      ],
      "metadata": {
        "id": "m23fz0EhQEGz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRBAXFBiNOF7"
      },
      "source": [
        "#### Step 1: Using the OpenAI python library\n",
        "\n",
        "â— Note: if you restarted your google Colab, you may need to re-run the first stup step back and the very top before coming back here â—"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mu56DHbbNOF7"
      },
      "outputs": [],
      "source": [
        "import os, secrets, requests\n",
        "import openai\n",
        "from requests.auth import HTTPBasicAuth\n",
        "\n",
        "#if using the Elastic AI proxy, then generate the correct API key\n",
        "if os.environ['ELASTIC_PROXY'] == \"True\":\n",
        "\n",
        "    if \"OPENAI_API_TYPE\" in os.environ: del os.environ[\"OPENAI_API_TYPE\"]\n",
        "\n",
        "    #generate and share \"your\" unique hash\n",
        "    os.environ['USER_HASH'] = secrets.token_hex(nbytes=6)\n",
        "    print(f\"Your unique user hash is: {os.environ['USER_HASH']}\")\n",
        "else:\n",
        "    openai.api_type = os.environ['OPENAI_API_TYPE']\n",
        "    openai.api_version = os.environ['OPENAI_API_VERSION']\n",
        "\n",
        "openai.api_key = os.environ['OPENAI_API_KEY']\n",
        "openai.api_base = os.environ['OPENAI_API_BASE']\n",
        "openai.default_model = os.environ['OPENAI_API_ENGINE']\n",
        "\n",
        "# Call the OpenAI ChatCompletion API\n",
        "def chatCompletion(messages):\n",
        "    if os.environ[\"ELASTIC_PROXY\"] == \"True\":\n",
        "        completion = openai.ChatCompletion.create(\n",
        "                        model=openai.default_model,\n",
        "                        max_tokens=100,\n",
        "                        messages=messages\n",
        "                      )\n",
        "    else:\n",
        "        completion = openai.ChatCompletion.create(\n",
        "                        engine=openai.default_model,\n",
        "                        max_tokens=100,\n",
        "                        messages=messages\n",
        "                      )\n",
        "    return completion\n",
        "\n",
        "def chatWithGPT(prompt, print_full_json=False):\n",
        "    response_text = chatCompletion([{\"role\": \"user\", \"content\": prompt}])\n",
        "\n",
        "    if print_full_json:\n",
        "      json_pretty(response_text)\n",
        "\n",
        "    return wrap_text(response_text.choices[0].message.content,70)\n",
        "\n",
        "## call it with the json debug output enabled\n",
        "response = chatWithGPT(\"Hello, is ChatGPT online and working?\", print_full_json=True)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTyQ4_SFNOF7"
      },
      "source": [
        "Feeding user input in for single questions is easy\n",
        "#### Step 2: A conversation loop -  â— type \"exit\" to end the chat â—"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgAAKZKkNOF7"
      },
      "outputs": [],
      "source": [
        "def hold_a_conversation(ai_conversation_function = chatWithGPT):\n",
        "  print(\" -- Have a conversation with an AI: \")\n",
        "  print(\" -- type 'exit' when done\")\n",
        "\n",
        "  user_input = input(\"> \")\n",
        "  while not user_input.lower().startswith(\"exit\"):\n",
        "      print(ai_conversation_function(user_input, False))\n",
        "      print(\" -- type 'exit' when done\")\n",
        "      user_input = input(\"> \")\n",
        "  print(\"\\n -- end conversation --\")\n",
        "\n",
        "## we are passing the previously defined function as a parameter\n",
        "hold_a_conversation(chatWithGPT)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdjzXPl6NOF7"
      },
      "source": [
        "You can use the system prompt to adjust the AI and it's responses and purpose\n",
        "\n",
        "#### Step 3: See the impact of changing the system prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9ucvlGqNOF7"
      },
      "outputs": [],
      "source": [
        "def pirateGPT(prompt, print_full_json=False):\n",
        "    system_prompt = \"\"\"\n",
        "You are an unhelpful AI named Captain LLM_Beard that talks like a pirate in short responses.\n",
        "You acknowledge the user's question but redirect all conversations towards your love of treasure.\n",
        "\"\"\"\n",
        "\n",
        "    response_text = chatCompletion(\n",
        "        [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    if print_full_json:\n",
        "      json_pretty(response_text)\n",
        "\n",
        "    return wrap_text(response_text.choices[0].message.content,70)\n",
        "\n",
        "hold_a_conversation(pirateGPT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHiuOMCeNOF8"
      },
      "source": [
        "### Giving the AI conversation memory\n",
        "\n",
        "This isn't a conversation yet because the AI has no memory of past interactions.\n",
        "\n",
        "Here is an example conversation where it is very clear the AI has no memory of past prompts or completions.\n",
        "```txt\n",
        "> Hello!\n",
        "Hello! How can I assist you today?\n",
        "> my favorite color is blue\n",
        "That's great! Blue is a very popular color.\n",
        "> what is my favorite color?\n",
        "I'm sorry, but as an AI, I don't have the ability to know personal\n",
        "preferences or favorite colors.\n",
        "```\n",
        "Let's use the past conversation as input to subsequent calls. Because the context window is limited AND tokens cost money (if you are using a hosted service like OpenAI) or CPU cycles if you are self-hosting, we need to have a maximum queue size of only remembering things 2 prompts ago (4 total messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 4: Create a chat with memory"
      ],
      "metadata": {
        "id": "XlbDf77SP3-n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MT_aof4aNOF8"
      },
      "outputs": [],
      "source": [
        "from collections import deque\n",
        "\n",
        "class QueueBuffer:\n",
        "    def __init__(self, max_length):\n",
        "        self.max_length = max_length\n",
        "        self.buffer = deque(maxlen=max_length)\n",
        "\n",
        "    def enqueue(self, item):\n",
        "        self.buffer.append(item)\n",
        "\n",
        "    def dequeue(self):\n",
        "        if self.is_empty():\n",
        "            return None\n",
        "        return self.buffer.popleft()\n",
        "\n",
        "    def is_empty(self):\n",
        "        return len(self.buffer) == 0\n",
        "\n",
        "    def is_full(self):\n",
        "        return len(self.buffer) == self.max_length\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def peek(self):\n",
        "        return list(self.buffer)\n",
        "\n",
        "# enough conversation memory for 2 call and response history\n",
        "memory_buffer = QueueBuffer(4)\n",
        "\n",
        "system_prompt = {\n",
        "          \"role\": \"system\",\n",
        "          \"content\": \"\"\"\n",
        "You are an AI named Cher Horowitz that speaks\n",
        "in 1990's valley girl dialect of English\"\"\"\n",
        "      }\n",
        "\n",
        "## utility function to print in a different color for debug output\n",
        "def print_light_blue(text):\n",
        "    print(f'\\033[94m{text}\\033[0m')\n",
        "\n",
        "## now with memory\n",
        "def cluelessGPT(prompt, print_full_json=False):\n",
        "\n",
        "  ## the API call will use the system prompt + the memory buffer\n",
        "  ## which ends with the user prompt\n",
        "  user_message = {\"role\": \"user\", \"content\": prompt}\n",
        "  memory_buffer.enqueue(user_message)\n",
        "\n",
        "  ## debug print the current AI memory\n",
        "  print_light_blue(\"Current memory\")\n",
        "  for m in memory_buffer.peek():\n",
        "      role = m.get(\"role\").strip()\n",
        "      content = m.get(\"content\").strip()\n",
        "      print_light_blue( f\"  {role} | {content}\")\n",
        "\n",
        "  ## when calling the AI we put the system prompt at the start\n",
        "  concatenated_message = [system_prompt] + memory_buffer.peek()\n",
        "\n",
        "  ## here is the request to the AI\n",
        "  completion = chatCompletion(concatenated_message)\n",
        "\n",
        "  response_text = completion.choices[0].message.content\n",
        "\n",
        "  ## don't forget to add the repsonse to the conversation memory\n",
        "  memory_buffer.enqueue({\"role\":\"assistant\", \"content\":response_text})\n",
        "\n",
        "  if print_full_json:\n",
        "    json_pretty(completion)\n",
        "\n",
        "  return wrap_text(response_text,70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTHELTKTNOF8"
      },
      "source": [
        "#### Step 5: Let's chat with a not so clueless chatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVztTDN6NOF8"
      },
      "outputs": [],
      "source": [
        "hold_a_conversation(cluelessGPT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK4UKihhNOF8"
      },
      "source": [
        "ðŸ›‘ Stop Here ðŸ›‘\n",
        "\n",
        "This Ends Lab 1-2\n",
        "<hr/>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lab 1-3: Data Redaction"
      ],
      "metadata": {
        "id": "92luYkkb5aaX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model import"
      ],
      "metadata": {
        "id": "YJH4Qofr5mm4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 1: Install and Import dependencies"
      ],
      "metadata": {
        "id": "CKKAZragT9ec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q eland elasticsearch transformers sentence_transformers python-dotenv\n",
        "\n",
        "from elasticsearch import Elasticsearch, helpers, exceptions\n",
        "from eland.ml.pytorch import PyTorchModel\n",
        "from eland.ml.pytorch.transformers import TransformerModel\n",
        "from getpass import getpass\n",
        "import tempfile\n",
        "import os\n",
        "from pprint import pprint"
      ],
      "metadata": {
        "id": "pp_RWV73aGAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 2: Create Elasticsearch Client Connection"
      ],
      "metadata": {
        "id": "JXj9kw_5UK_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'ELASTIC_CLOUD_ID' in os.environ:\n",
        "  es = Elasticsearch(\n",
        "    cloud_id=os.environ['ELASTIC_CLOUD_ID'],\n",
        "    api_key=(os.environ['ELASTIC_APIKEY_ID'], os.environ['ELASTIC_APIKEY_SECRET']),\n",
        "    request_timeout=30\n",
        "  )\n",
        "elif 'ELASTIC_URL' in os.environ:\n",
        "  es = Elasticsearch(\n",
        "    os.environ['ELASTIC_URL'],\n",
        "    api_key=(os.environ['ELASTIC_APIKEY_ID'], os.environ['ELASTIC_APIKEY_SECRET']),\n",
        "    request_timeout=30\n",
        "  )\n",
        "else:\n",
        "  print(\"env needs to set either ELASTIC_CLOUD_ID or ELASTIC_URL\")"
      ],
      "metadata": {
        "id": "m-_r6lROasNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 3: Define the model import function"
      ],
      "metadata": {
        "id": "nuiUxbCkYS3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model_id, task_type):\n",
        "  with tempfile.TemporaryDirectory() as tmp_dir:\n",
        "    print(f\"Loading HuggingFace transformer tokenizer and model [{model_id}] for task [{task_type}]\" )\n",
        "\n",
        "    tm = TransformerModel(model_id=model_id, task_type=task_type)\n",
        "    model_path, config, vocab_path = tm.save(tmp_dir)\n",
        "\n",
        "    ptm = PyTorchModel(es, tm.elasticsearch_model_id())\n",
        "    model_exists = es.options(ignore_status=404).ml.get_trained_models(model_id=ptm.model_id).meta.status == 200\n",
        "\n",
        "    if model_exists:\n",
        "      print(\"Model has already been imported\")\n",
        "    else:\n",
        "      print(\"Importing model\")\n",
        "      ptm.import_model(model_path=model_path, config_path=None, vocab_path=vocab_path, config=config)\n",
        "      print(\"Starting model deployment\")\n",
        "      ptm.start()\n",
        "      print(f\"Model successfully imported with id '{ptm.model_id}'\")"
      ],
      "metadata": {
        "id": "yn7sULLEYW6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 4: Import the model"
      ],
      "metadata": {
        "id": "oYxSkBSJZaAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "load_model(\"dslim/bert-base-NER\", \"ner\")"
      ],
      "metadata": {
        "id": "A_AL319-dmKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the ingest pipeline\n",
        "\n",
        "We will use the [Elasticsearch Ingest Pipelines](https://www.elastic.co/guide/en/elasticsearch/reference/current/ingest.html) to redact data before it is written to Elasticsearch. These pipelines can also be used to update data in existing indices or for reindexing.\n",
        "\n",
        "This pipeline:\n",
        "- Uses the [inference processor](https://www.elastic.co/guide/en/elasticsearch/reference/current/inference-processor.html) to call the NER model loaded in Part 1 and map the document's `message` field to the field expected by the model: `text_field`.\n",
        "- Uses the [Painless scripting language](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-scripting-painless.html) from within a [script processor](https://www.elastic.co/guide/en/elasticsearch/reference/current/script-processor.html) to replace the model-detected entities stored in the `ml.inference.entities` array with their class name, and store it within a **new** document field: `redacted`.\n",
        "- Uses the [redact processor](https://www.elastic.co/guide/en/elasticsearch/reference/current/redact-processor.html) to identify and redact any supported patterns found within the new redacted field, as well as identifying and redacting a set of custom patterns.\n",
        "- Removes the `ml` fields added to the document by the inference processor via the [remove processor](https://www.elastic.co/guide/en/elasticsearch/reference/current/remove-processor.html) as they're no longer needed.\n",
        "- Defines a failure condition to capture any errors, just in case we have them.\n",
        "\n",
        "**NOTE:** As of 8.9, the redact processor is a Technical Preview.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Lg7MWpurb7g2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "body = {\n",
        "   \"processors\": [\n",
        "    {\n",
        "       \"inference\": {\n",
        "         \"model_id\": \"dslim__bert-base-ner\",\n",
        "         \"field_map\": {\n",
        "           \"message\": \"text_field\"\n",
        "         }\n",
        "       }\n",
        "    },\n",
        "    {\n",
        "       \"script\": {\n",
        "         \"lang\": \"painless\",\n",
        "         \"source\": \"\"\"\n",
        "String msg = ctx['message'];\n",
        "for (item in ctx['ml']['inference']['entities'])\n",
        "  msg = msg.replace(item['entity'], '<' + item['class_name'] + '>');\n",
        "ctx['redacted'] = msg;\n",
        "\"\"\"\n",
        "       }\n",
        "    },\n",
        "    {\n",
        "       \"redact\": {\n",
        "          \"field\": \"redacted\",\n",
        "          \"patterns\": [\n",
        "            \"%{EMAILADDRESS:EMAIL}\",\n",
        "            \"%{IP:IP_ADDRESS}\",\n",
        "            \"%{CREDIT_CARD:CREDIT_CARD}\",\n",
        "            \"%{SSN:SSN}\",\n",
        "            \"%{PHONE:PHONE}\"\n",
        "      ],\n",
        "          \"pattern_definitions\": {\n",
        "            \"CREDIT_CARD\": \"\\\\d{4}[ -]\\\\d{4}[ -]\\\\d{4}[ -]\\\\d{4}\",\n",
        "            \"SSN\": \"\\\\d{3}-\\\\d{2}-\\\\d{4}\",\n",
        "            \"PHONE\": \"\\\\d{3}-\\\\d{3}-\\\\d{4}\"\n",
        "          }\n",
        "       }\n",
        "    },\n",
        "    {\n",
        "       \"remove\": {\n",
        "         \"field\": [\n",
        "           \"ml\"\n",
        "         ],\n",
        "         \"ignore_missing\": True,\n",
        "         \"ignore_failure\": True\n",
        "       }\n",
        "    }\n",
        "  ],\n",
        "  \"on_failure\": [\n",
        "    {\n",
        "       \"set\": {\n",
        "         \"field\": \"failure\",\n",
        "         \"value\": \"pii_script-redact\"\n",
        "       }\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "\n",
        "#es.ingest.put_pipeline(id='redact', body=body)"
      ],
      "metadata": {
        "id": "4_GKI4ZQd5zw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Test the pipeline\n",
        "\n",
        "Does it work?\n",
        "\n",
        "Let's use the [Simulate Pipeline API](https://www.elastic.co/guide/en/elasticsearch/reference/current/simulate-pipeline-api.html) to find out."
      ],
      "metadata": {
        "id": "Dcv-hhwqlkjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs = [\n",
        "  {\n",
        "      \"_source\": {\n",
        "          \"message\": \"John Smith lives at 123 Main St. Highland Park, CO. His email address \"\\\n",
        "          \"is jsmith123@email.com and his phone number is 412-189-9043.  I found his social \"\\\n",
        "          \"security number, it is 942-00-1243. Oh btw, his credit card is 1324-8374-0978-2819 \"\\\n",
        "          \"and his gateway IP is 192.168.1.2\"\n",
        "      }\n",
        "  },\n",
        "  {\n",
        "      \"_source\": {\n",
        "          \"message\": \"I had a call with Jane yesterday, she suggested we talk with John \"\\\n",
        "          \"from Global Systems. Their office is in Springfield\"\n",
        "      }\n",
        "  }\n",
        "]\n",
        "\n",
        "pprint(es.ingest.simulate(id='redact', docs=docs).body)"
      ],
      "metadata": {
        "id": "HaXVRsyLhr22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A good next step after validating the pipeline performs as needed is to create role permissions to limit who can see the original `message` vs the new `redacted` version.\n",
        "\n",
        "Check out the [Field Level Security documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/field-level-security.html)"
      ],
      "metadata": {
        "id": "IOkgdcu9mCIS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ›‘ Stop Here ðŸ›‘\n",
        "\n",
        "This Ends Lab 1-3\n",
        "<hr/>"
      ],
      "metadata": {
        "id": "KlWAaHh8Qnx9"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}