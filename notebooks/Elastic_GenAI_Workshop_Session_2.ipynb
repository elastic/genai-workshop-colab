{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 2"
      ],
      "metadata": {
        "id": "nFbQGw2POViM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup Environment\n",
        "The following code loads the environment variables required to run this notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "FILE=\"Elastic_GenAI_Workshop_session_2\"\n",
        "\n",
        "! pip install -qqq git+https://github.com/elastic/notebook-workshop-loader.git@main\n",
        "from notebookworkshoploader import loader\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "if os.path.isfile(\"../env\"):\n",
        "    load_dotenv(\"../env\", override=True)\n",
        "    print('Successfully loaded environment variables from local env file')\n",
        "else:\n",
        "    loader.load_remote_env(file=FILE, env_url=\"https://notebook-workshop-setup.elasticsa.co\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "CZO8krnZMLZj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Environment"
      ],
      "metadata": {
        "id": "gM0zjk2VR2OR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit \"openai<1.0.0\" elasticsearch elastic-apm inquirer python-dotenv\n",
        "!npm install localtunnel --loglevel=error\n",
        "\n",
        "import os, inquirer, re, secrets, requests\n",
        "import streamlit as st\n",
        "import openai\n",
        "\n",
        "from IPython.display import display\n",
        "from ipywidgets import widgets\n",
        "from pprint import pprint\n",
        "from elasticsearch import Elasticsearch\n",
        "from string import Template\n",
        "from requests.auth import HTTPBasicAuth\n",
        "\n",
        "#if using the Elastic AI proxy, then generate the correct API key\n",
        "if os.environ['ELASTIC_PROXY'] == \"True\":\n",
        "\n",
        "  #remove the api type variable: it's a must when using the proxy\n",
        "  if \"OPENAI_API_TYPE\" in os.environ: del os.environ[\"OPENAI_API_TYPE\"]\n"
      ],
      "metadata": {
        "id": "Ln-8SRvAI-jS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install localtunnel\n",
        "This is a Colab-specific requirement to allow us to connect to a Streamlit app we're building from within Colab"
      ],
      "metadata": {
        "id": "TkZrWNJecO7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel --loglevel=error"
      ],
      "metadata": {
        "id": "02NstyujMogn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Elasticsearch client connection"
      ],
      "metadata": {
        "id": "wEOV5KlycEmV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'ELASTIC_CLOUD_ID' in os.environ:\n",
        "  es = Elasticsearch(\n",
        "    cloud_id=os.environ['ELASTIC_CLOUD_ID'],\n",
        "    api_key=(os.environ['ELASTIC_APIKEY_ID'], os.environ['ELASTIC_APIKEY_SECRET']),\n",
        "    request_timeout=30\n",
        "  )\n",
        "elif 'ELASTIC_URL' in os.environ:\n",
        "  es = Elasticsearch(\n",
        "    os.environ['ELASTIC_URL'],\n",
        "    api_key=(os.environ['ELASTIC_APIKEY_ID'], os.environ['ELASTIC_APIKEY_SECRET']),\n",
        "    request_timeout=30\n",
        "  )\n",
        "else:\n",
        "  print(\"env needs to set either ELASTIC_CLOUD_ID or ELASTIC_URL\")"
      ],
      "metadata": {
        "id": "AUIB9n9ccGAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 2-1\n",
        "- Chunking (simplified example)\n",
        "- Generating embeddings\n",
        "- Perform kNN search"
      ],
      "metadata": {
        "id": "E0uQujqZclf0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chunking\n",
        "Simplfied example"
      ],
      "metadata": {
        "id": "fQQ-GAcOecAE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "body_content = \"Elastic Docs › Elasticsearch Guide [8.8] « Searchable snapshots Elasticsearch security principles » Secure the Elastic Stack edit The Elastic Stack is comprised of many moving parts. There are the Elasticsearch nodes that form the cluster, plus Logstash instances, Kibana instances, Beats agents, and clients all communicating with the cluster. To keep your cluster safe, adhere to the Elasticsearch security principles . The first principle is to run Elasticsearch with security enabled. Configuring security can be complicated, so we made it easy to start the Elastic Stack with security enabled and configured . For any new clusters, just start Elasticsearch to automatically enable password protection, secure internode communication with Transport Layer Security (TLS), and encrypt connections between Elasticsearch and Kibana. If you have an existing, unsecured cluster (or prefer to manage security on your own), you can manually enable and configure security to secure Elasticsearch clusters and any clients that communicate with your clusters. You can also implement additional security measures, such as role-based access control, IP filtering, and auditing. Enabling security protects Elasticsearch clusters by: Preventing unauthorized access with password protection, role-based access control, and IP filtering. Preserving the integrity of your data with SSL/TLS encryption. Maintaining an audit trail so you know who’s doing what to your cluster and the data it stores. If you plan to run Elasticsearch in a Federal Information Processing Standard (FIPS) 140-2 enabled JVM, see FIPS 140-2 . Preventing unauthorized access edit To prevent unauthorized access to your Elasticsearch cluster, you need a way to authenticate users in order to validate that a user is who they claim to be. For example, making sure that only the person named Kelsey Andorra can sign in as the user kandorra . The Elasticsearch security features provide a standalone authentication mechanism that enables you to quickly password-protect your cluster. If you’re already using LDAP, Active Directory, or PKI to manage users in your organization, the security features integrate with those systems to perform user authentication. In many cases, authenticating users isn’t enough. You also need a way to control what data users can access and what tasks they can perform. By enabling the Elasticsearch security features, you can authorize users by assigning access privileges to roles and assigning those roles to users. Using this role-based access control mechanism (RBAC), you can limit the user kandorra to only perform read operations on the events index restrict access to all other indices. The security features also enable you to restrict the nodes and clients that can connect to the cluster based on IP filters . You can block and allow specific IP addresses, subnets, or DNS domains to control network-level access to a cluster. See User authentication and User authorization . Preserving data integrity and confidentiality edit A critical part of security is keeping confidential data secured. Elasticsearch has built-in protections against accidental data loss and corruption. However, there’s nothing to stop deliberate tampering or data interception. The Elastic Stack security features use TLS to preserve the integrity of your data against tampering, while also providing confidentiality by encrypting communications to, from, and within the cluster. For even greater protection, you can increase the encryption strength . See Configure security for the Elastic Stack . Maintaining an audit trail edit Keeping a system secure takes vigilance. By using Elastic Stack security features to maintain an audit trail, you can easily see who is accessing your cluster and what they’re doing. You can configure the audit level, which accounts for the type of events that are logged. These events include failed authentication attempts, user access denied, node connection denied, and more. By analyzing access patterns and failed attempts to access your cluster, you can gain insights into attempted attacks and data breaches. Keeping an auditable log of the activity in your cluster can also help diagnose operational issues. See Enable audit logging . « Searchable snapshots Elasticsearch security principles » Most Popular Video Get Started with Elasticsearch Video Intro to Kibana Video ELK for Logs & Metrics\"\n",
        "print (\"The length of the paragraph is %s characters\" % len (body_content))"
      ],
      "metadata": {
        "id": "RCArFD3kecTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are many ways to split text. We can split on individual characters, spaces, at a set length, using a library like langchain, or using a tokenizer, to name a few ways.\n",
        "\n",
        "For this simple example we are going to split on dot+space \". \", essentially spliting individual sentences."
      ],
      "metadata": {
        "id": "-NGORlTWncWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chunked_content = [chunk for chunk in re.split('\\. ',  body_content)]\n",
        "chunk = chunked_content[0] # We'll use this later\n",
        "print (\"There are now %s sentence chunks.\\nThe first element is:'%s'\" % (len(chunked_content), chunk))"
      ],
      "metadata": {
        "id": "sVu0PcLzolSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO Talk about tokens"
      ],
      "metadata": {
        "id": "D7NHs6s2qYii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the \"tokens\" from the first chunk\n",
        "chunk.split()"
      ],
      "metadata": {
        "id": "UwAOqksbrOUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate embeddings"
      ],
      "metadata": {
        "id": "4DL2MkCprTBe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to pass our text to an embedding model to generate vectors.\n",
        "\n",
        "Models have pre-definied token limits which restrict the amount of text (tokens really) that can be processed into vectors."
      ],
      "metadata": {
        "id": "i3xI7Rg8rYOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "es_model_id = 'sentence-transformers__msmarco-minilm-l-12-v3'"
      ],
      "metadata": {
        "id": "Co_jW0DevAUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunk"
      ],
      "metadata": {
        "id": "lXVW_fOpsr0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs =  [\n",
        "    {\n",
        "      \"text_field\": chunk\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "K8MOh-2Fqbr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_vector = es.ml.infer_trained_model(model_id=es_model_id, docs=docs, )"
      ],
      "metadata": {
        "id": "Q-n1lTvAvVht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_doc = {\n",
        "  \"_index\": \"chunker\",\n",
        "  \"_id\": \"64837860d86b1293a9a5f620-0\",\n",
        "  \"_source\": {\n",
        "      \"chunk\" : chunk,\n",
        "      \"chunk-vector\" : chunk_vector['inference_results'][0]['predicted_value'],\n",
        "      \"body_content\" : body_content\n",
        "  }\n",
        "}\n",
        "\n",
        "pprint(vector_doc)"
      ],
      "metadata": {
        "id": "1XHbhuX4v3ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exceeding the model's token limit"
      ],
      "metadata": {
        "id": "LEK9CFy9w-6d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at what happens when we exceed the model's token limit"
      ],
      "metadata": {
        "id": "Ndjrwim0XS-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_paragraph =  [\n",
        "    {\n",
        "      \"text_field\": body_content\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "ZVIaFOG-yaKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_vector = es.ml.infer_trained_model(model_id=es_model_id, docs=full_paragraph, )\n",
        "print(\"When the token size exceeds the model's max token limit, the value of `is_truncated` will return True\")\n",
        "print('We exceeded the model token limit: %s' % chunk_vector['inference_results'][0]['is_truncated'])"
      ],
      "metadata": {
        "id": "6msjDqmZyaKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that the model still processed the tokens up to it's limit, then simply truncated (ignored) any tokens longer than that.\n",
        "\n",
        "Elasticsearch returns a `is_truncated : True` key:value to let you know the embedding returned is not for the full text."
      ],
      "metadata": {
        "id": "qYMM629tXYF8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Querying with hybrid vector search"
      ],
      "metadata": {
        "id": "tIbhhCa4oKPO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will run through an example of searching with approximate kNN vector search combined with BM25 text search combing the results with rrf.\n",
        "\n",
        "This is the type of query that will power the UI we will use in lab 2-2"
      ],
      "metadata": {
        "id": "cFNyCWCWp_nI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search_with_knn(query_text, es):\n",
        "    # Elasticsearch query (BM25) and kNN configuration for rrf hybrid search\n",
        "\n",
        "    query = {\n",
        "        \"bool\": {\n",
        "            \"must\": [{\n",
        "                \"match\": {\n",
        "                    \"body_content\": {\n",
        "                        \"query\": query_text\n",
        "                    }\n",
        "                }\n",
        "            }],\n",
        "            \"filter\": [{\n",
        "              \"term\": {\n",
        "                \"url_path_dir3\": \"elasticsearch\"\n",
        "              }\n",
        "            }]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    knn = [\n",
        "    {\n",
        "      \"field\": \"chunk-vector\",\n",
        "      \"k\": 10,\n",
        "      \"num_candidates\": 10,\n",
        "      \"filter\": {\n",
        "        \"bool\": {\n",
        "          \"filter\": [\n",
        "            {\n",
        "              \"range\": {\n",
        "                \"chunklength\": {\n",
        "                  \"gte\": 0\n",
        "                }\n",
        "              }\n",
        "            },\n",
        "            {\n",
        "              \"term\": {\n",
        "                \"url_path_dir3\": \"elasticsearch\"\n",
        "              }\n",
        "            }\n",
        "          ]\n",
        "        }\n",
        "      },\n",
        "      \"query_vector_builder\": {\n",
        "        \"text_embedding\": {\n",
        "          \"model_id\": \"sentence-transformers__msmarco-minilm-l-12-v3\",\n",
        "          \"model_text\": query_text\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  ]\n",
        "\n",
        "    rank = {\n",
        "       \"rrf\": {\n",
        "       }\n",
        "   }\n",
        "\n",
        "    fields= [\n",
        "        \"title\",\n",
        "        \"url\",\n",
        "        \"body_content\"\n",
        "      ]\n",
        "\n",
        "    resp = es.search(index=os.environ['ELASTIC_INDEX_DOCS'],\n",
        "                     query=query,\n",
        "                     knn=knn,\n",
        "                     rank=rank,\n",
        "                     fields=fields,\n",
        "                     size=1,\n",
        "                     source=False)\n",
        "\n",
        "    return resp\n",
        "\n",
        "query = 'How do I start Elastic with Security Enabled?'\n",
        "response = search_with_knn(query, es)\n",
        "pprint(response['hits'])"
      ],
      "metadata": {
        "id": "yVqwwfskoKqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 2-2\n",
        "RAG"
      ],
      "metadata": {
        "id": "tCsxLt8Yc2LI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify our Elasticsearch connection is still active\n",
        "If you receive an error, rerun the cells in the Setup section above"
      ],
      "metadata": {
        "id": "ELRm5s9pc5Jg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(es.info()['tagline']) # should return cluster info"
      ],
      "metadata": {
        "id": "Isssrm07dKMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main Script\n",
        "Running this cell will write a file named `app.py` into the Colab environment"
      ],
      "metadata": {
        "id": "t7RmurdZNPg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import os\n",
        "import streamlit as st\n",
        "import openai\n",
        "from elasticsearch import Elasticsearch\n",
        "from string import Template\n",
        "import elasticapm\n",
        "\n",
        "# Configure OpenAI client\n",
        "openai.api_key = os.environ['OPENAI_API_KEY']\n",
        "openai.api_base = os.environ['OPENAI_API_BASE']\n",
        "openai.default_model = os.environ['OPENAI_API_ENGINE']\n",
        "openai.verify_ssl_certs = False\n",
        "\n",
        "# Configure APM and Elasticsearch clients\n",
        "@st.cache_resource\n",
        "def initElastic():\n",
        "    os.environ['ELASTIC_APM_SERVICE_NAME'] = \"genai_workshop_lab_2-2\"\n",
        "    apmclient = elasticapm.Client()\n",
        "    elasticapm.instrument()\n",
        "\n",
        "    if 'ELASTIC_CLOUD_ID' in os.environ:\n",
        "      es = Elasticsearch(\n",
        "        cloud_id=os.environ['ELASTIC_CLOUD_ID'],\n",
        "        api_key=(os.environ['ELASTIC_APIKEY_ID'], os.environ['ELASTIC_APIKEY_SECRET']),\n",
        "        request_timeout=30\n",
        "      )\n",
        "    else:\n",
        "      es = Elasticsearch(\n",
        "        os.environ['ELASTIC_URL'],\n",
        "        api_key=(os.environ['ELASTIC_APIKEY_ID'], os.environ['ELASTIC_APIKEY_SECRET']),\n",
        "        request_timeout=30\n",
        "      )\n",
        "\n",
        "    if os.environ['ELASTIC_PROXY'] != \"True\":\n",
        "        openai.api_type = os.environ['OPENAI_API_TYPE']\n",
        "        openai.api_version = os.environ['OPENAI_API_VERSION']\n",
        "\n",
        "    return apmclient, es\n",
        "apmclient, es = initElastic()\n",
        "\n",
        "# Set our data index\n",
        "index = os.environ['ELASTIC_INDEX_DOCS']\n",
        "\n",
        "# Run an Elasticsearch query using BM25 relevance scoring\n",
        "@elasticapm.capture_span(\"bm25_search\")\n",
        "def search_bm25(query_text, es):\n",
        "    query = {\n",
        "        \"match\": {\n",
        "            \"body_content\": query_text\n",
        "        }\n",
        "    }\n",
        "\n",
        "    fields= [\n",
        "        \"title\",\n",
        "        \"url\",\n",
        "        \"position\",\n",
        "        \"body_content\"\n",
        "      ]\n",
        "\n",
        "    collapse= {\n",
        "      \"field\": \"title.enum\"\n",
        "    }\n",
        "\n",
        "    resp = es.search(index=index,\n",
        "                     query=query,\n",
        "                     fields=fields,\n",
        "                     collapse=collapse,\n",
        "                     size=1,\n",
        "                     source=False)\n",
        "\n",
        "    body = resp['hits']['hits'][0]['fields']['body_content'][0]\n",
        "    url = resp['hits']['hits'][0]['fields']['url'][0]\n",
        "\n",
        "    return body, url\n",
        "\n",
        "# Run an Elasticsearch query using ELSER relevance scoring\n",
        "@elasticapm.capture_span(\"elser_search\")\n",
        "def search_elser(query_text, es):\n",
        "    query = {\n",
        "      \"text_expansion\": {\n",
        "        \"ml.inference.chunk_expanded.tokens\": {\n",
        "          \"model_id\": \".elser_model_1\",\n",
        "          \"model_text\": query_text\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "\n",
        "    fields = [\n",
        "      \"title\",\n",
        "      \"url\",\n",
        "      \"position\",\n",
        "      \"body_content\"\n",
        "    ]\n",
        "\n",
        "    collapse = {\n",
        "      \"field\": \"title.enum\"\n",
        "    }\n",
        "\n",
        "    resp = es.search(index=index,\n",
        "                     query=query,\n",
        "                     fields=fields,\n",
        "                     collapse=collapse,\n",
        "                     size=1,\n",
        "                     source=False)\n",
        "\n",
        "    body = resp['hits']['hits'][0]['fields']['body_content'][0]\n",
        "    url = resp['hits']['hits'][0]['fields']['url'][0]\n",
        "\n",
        "    return body, url\n",
        "\n",
        "# Run an Elasticsearch query using hybrid RRF scoring of KNN and BM25\n",
        "@elasticapm.capture_span(\"knn_search\")\n",
        "def search_knn(query_text, es):\n",
        "    query = {\n",
        "        \"bool\": {\n",
        "            \"must\": [{\n",
        "                \"match\": {\n",
        "                    \"body_content\": {\n",
        "                        \"query\": query_text\n",
        "                    }\n",
        "                }\n",
        "            }],\n",
        "            \"filter\": [{\n",
        "              \"term\": {\n",
        "                \"url_path_dir3\": \"elasticsearch\"\n",
        "              }\n",
        "            }]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    knn = [\n",
        "    {\n",
        "      \"field\": \"chunk-vector\",\n",
        "      \"k\": 10,\n",
        "      \"num_candidates\": 10,\n",
        "      \"filter\": {\n",
        "        \"bool\": {\n",
        "          \"filter\": [\n",
        "            {\n",
        "              \"range\": {\n",
        "                \"chunklength\": {\n",
        "                  \"gte\": 0\n",
        "                }\n",
        "              }\n",
        "            },\n",
        "            {\n",
        "              \"term\": {\n",
        "                \"url_path_dir3\": \"elasticsearch\"\n",
        "              }\n",
        "            }\n",
        "          ]\n",
        "        }\n",
        "      },\n",
        "      \"query_vector_builder\": {\n",
        "        \"text_embedding\": {\n",
        "          \"model_id\": \"sentence-transformers__msmarco-minilm-l-12-v3\",\n",
        "          \"model_text\": query_text\n",
        "        }\n",
        "      }\n",
        "    }]\n",
        "\n",
        "    rank = {\n",
        "       \"rrf\": {\n",
        "       }\n",
        "    }\n",
        "\n",
        "    fields= [\n",
        "        \"title\",\n",
        "        \"url\",\n",
        "        \"position\",\n",
        "        \"url_path_dir3\",\n",
        "        \"body_content\"\n",
        "      ]\n",
        "\n",
        "    resp = es.search(index=index,\n",
        "                     query=query,\n",
        "                     knn=knn,\n",
        "                     rank=rank,\n",
        "                     fields=fields,\n",
        "                     size=10,\n",
        "                     source=False)\n",
        "\n",
        "    body = resp['hits']['hits'][0]['fields']['body_content'][0]\n",
        "    url = resp['hits']['hits'][0]['fields']['url'][0]\n",
        "\n",
        "    return body, url\n",
        "\n",
        "def truncate_text(text, max_tokens):\n",
        "    tokens = text.split()\n",
        "    if len(tokens) <= max_tokens:\n",
        "        return text\n",
        "\n",
        "    return ' '.join(tokens[:max_tokens])\n",
        "\n",
        "# Generate a response from ChatGPT based on the given prompt\n",
        "def chat_gpt(prompt, max_tokens=1024, max_context_tokens=4000, safety_margin=5, sys_content=None):\n",
        "\n",
        "    # Truncate the prompt content to fit within the model's context length\n",
        "    truncated_prompt = truncate_text(prompt, max_context_tokens - max_tokens - safety_margin)\n",
        "\n",
        "    # Make the right OpenAI call depending on the API we're using\n",
        "    if(os.environ[\"ELASTIC_PROXY\"] == \"True\"):\n",
        "      response = openai.ChatCompletion.create(model=openai.default_model,\n",
        "                                              temperature=0,\n",
        "                                              messages=[{\"role\": \"system\", \"content\": sys_content},\n",
        "                                                        {\"role\": \"user\", \"content\": truncated_prompt}]\n",
        "                                              )\n",
        "    else:\n",
        "      response = openai.ChatCompletion.create(engine=openai.default_model,\n",
        "                                              temperature=0,\n",
        "                                              messages=[{\"role\": \"system\", \"content\": sys_content},\n",
        "                                                        {\"role\": \"user\", \"content\": truncated_prompt}]\n",
        "                                              )\n",
        "\n",
        "\n",
        "    # APM: add metadata labels of data we want to capture\n",
        "    elasticapm.label(model = openai.default_model)\n",
        "    elasticapm.label(prompt = prompt)\n",
        "    elasticapm.label(total_tokens = response[\"usage\"][\"total_tokens\"])\n",
        "    elasticapm.label(prompt_tokens = response[\"usage\"][\"prompt_tokens\"])\n",
        "    elasticapm.label(response_tokens = response[\"usage\"][\"completion_tokens\"])\n",
        "    if 'USER_HASH' in os.environ: elasticapm.label(user = os.environ['USER_HASH'])\n",
        "\n",
        "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "def toLLM(resp, url, usr_prompt, sys_prompt, neg_resp, show_prompt):\n",
        "    prompt_template = Template(usr_prompt)\n",
        "    prompt_formatted = prompt_template.substitute(query=query, resp=resp, negResponse=negResponse)\n",
        "    answer = chat_gpt(prompt_formatted, sys_content=sys_prompt)\n",
        "\n",
        "    # Display response from LLM\n",
        "    st.header('Response from LLM')\n",
        "    st.markdown(answer.strip())\n",
        "\n",
        "    # We don't need to return a reference URL if it wasn't useful\n",
        "    if not negResponse in answer:\n",
        "        st.write(url)\n",
        "\n",
        "    # Display full prompt if checkbox was selected\n",
        "    if show_prompt:\n",
        "        st.divider()\n",
        "        st.subheader('Full prompt sent to LLM')\n",
        "        prompt_formatted\n",
        "\n",
        "# Prompt Defaults\n",
        "prompt_default = \"\"\"Answer this question: $query\n",
        "Using only the information from this Elastic Doc: $resp\n",
        "Format the answer in complete markdown code format\n",
        "If the answer is not contained in the supplied doc reply '$negResponse' and nothing else\"\"\"\n",
        "\n",
        "system_default = 'You are a helpful assistant.'\n",
        "neg_default = \"I'm unable to answer the question based on the information I have from Elastic Docs.\"\n",
        "\n",
        "\n",
        "''' Main chat form\n",
        "'''\n",
        "st.title(\"ElasticDocs GPT\")\n",
        "\n",
        "with st.form(\"chat_form\"):\n",
        "\n",
        "    query = st.text_input(\"Ask the Elastic Documentation a question: \", placeholder='I want to secure my elastic cluster')\n",
        "\n",
        "    with st.expander(\"Show Prompt Override Inputs\"):\n",
        "        # Inputs for system and User prompt override\n",
        "        sys_prompt = st.text_area(\"create an alernative system prompt\", placeholder=system_default, value=system_default)\n",
        "        usr_prompt = st.text_area(\"create an alternative user prompt required -> \\$query, \\$resp, \\$negResponse\",\n",
        "                                   placeholder=prompt_default, value=prompt_default )\n",
        "\n",
        "        # Default Response when criteria are not met\n",
        "        negResponse = st.text_area(\"Create an alternative negative response\", placeholder = neg_default, value=neg_default)\n",
        "\n",
        "    show_full_prompt = st.checkbox('Show Full Prompt Sent to LLM')\n",
        "\n",
        "    # Query Submit Buttons\n",
        "    col1, col2, col3 = st.columns(3)\n",
        "    with col1:\n",
        "        bm25_button = st.form_submit_button(\"Use BM25\")\n",
        "    with col2:\n",
        "        knn_button = st.form_submit_button(\"Use kNN\")\n",
        "    with col3:\n",
        "        elser_button = st.form_submit_button(\"Use ELSER\")\n",
        "\n",
        "if elser_button:\n",
        "    apmclient.begin_transaction(\"query\")\n",
        "    elasticapm.label(search_method = \"elser\")\n",
        "    elasticapm.label(query = query)\n",
        "\n",
        "    resp, url = search_elser(query, es) # run ELSER query\n",
        "    toLLM(resp, url, usr_prompt, sys_prompt, negResponse, show_full_prompt)\n",
        "\n",
        "    apmclient.end_transaction(\"query\", \"success\")\n",
        "if knn_button:\n",
        "    apmclient.begin_transaction(\"query\")\n",
        "    elasticapm.label(search_method = \"knn\")\n",
        "    elasticapm.label(query = query)\n",
        "\n",
        "    resp, url = search_knn(query, es) # run kNN hybrid query\n",
        "    toLLM(resp, url, usr_prompt, sys_prompt, negResponse, show_full_prompt)\n",
        "\n",
        "    apmclient.end_transaction(\"query\", \"success\")\n",
        "if bm25_button:\n",
        "    apmclient.begin_transaction(\"query\")\n",
        "    elasticapm.label(search_method = \"bm25\")\n",
        "    elasticapm.label(query = query)\n",
        "\n",
        "    resp, url = search_bm25(query, es) # run kNN hybrid query\n",
        "    toLLM(resp, url, usr_prompt, sys_prompt, negResponse, show_full_prompt)\n",
        "\n",
        "    apmclient.end_transaction(\"query\", \"success\")\n"
      ],
      "metadata": {
        "id": "Wc02OlkpOSd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Streamlit\n",
        "Running this cell will start local tunnel and generate a random URL\n",
        "\n",
        "Copy the IP address on the first line then open the generated URL and paste it in the input box \"Endpoint IP\"\n",
        "\n",
        "This will then start the Streamlit app"
      ],
      "metadata": {
        "id": "Wu0KfS0ESf6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py &>/content/logs.txt & npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ],
      "metadata": {
        "id": "cHIHFID3NBXa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}